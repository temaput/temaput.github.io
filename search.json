[
  {
    "objectID": "projects-list.html",
    "href": "projects-list.html",
    "title": "Projects List",
    "section": "",
    "text": "Here is a list of projects I have worked on, specifically selected to include both those that were published and those that hold personal significance to me.\nArtem Putilov, 2023"
  },
  {
    "objectID": "projects-list.html#completed-projects",
    "href": "projects-list.html#completed-projects",
    "title": "Projects List",
    "section": "Completed Projects",
    "text": "Completed Projects\n\nProjectX - custom ERP solution for a talent agency\nCreated complete architecture, also worked as project manager and team lead.\n\nMicroservices architecture (NestJS)\nCQRS and Event Sourcing (MongoDB as main eventlog, Elastic as Projections db, RabbitMQ as broker)\nAdmin Dashboard built from scratch on React Bootstrap\nComplex filter builder solution powered by Elastic Search\n\n\n\nWeb Quizz Game\nIn 2021 I worked solo on quickly developing this SPA. The app is a Quizz Game built using NextJS, Firebase and ReTool as an admin panel. Whole project from scratch was done in 2 weeks time.\n\n\nElias Gameplan App\nIn 2021 I worked as TeamLead and frontend developer on this React.Native app. We built it from scratch.\n\n\nUVE Artiste App\nIn 2020 I developed this React.Native app from scratch. It uses Firebase for Authentication and messaging.\n\n\nUVE Client App\nI started working on this React.Native app in 2019. Added a lot of functionality, fixed a lot of bugs since than. We created a Web portal based on this iPad app using react-native-web. Later I added MFA there using poorly documented and Firebase MFA feature. I had to extend Javascript Firebase SDK, because it was missing MFA feature.\n\n\nKROST Mobile App\nThis application was created by another team. My job was to introduce several new features including whole bunch of semi-automatic push notifications, notifications centre a to upgrade deployment infrastructure. I upgraded IOS version (Swift) and backend (Django). For the new deployment scheme I had to remotely create private cloud with Kubernetes.\n\n\nCoffee Brewing Guide for Amazon Alexa\nHere is the Amazon Alexa skill that I developed for London Coffee Brewery. I worked together with wonderfull voice ux designer. Together we created complex guide for multiple modern coffee machines. The main script introduced state machine and was written on Typescript using Skills SDK. Hosted on AWS Lambda.\n\n\nSnapchat game\nHere is a game I wrote on Snapchat Lens studio. It’s a classic space-invaders where you move your ship and open fire with your face!\n\n\nTemnikova AR game\nThis game was developed by my company. My job was creating a “serverless” backend using Firebase facilities (RTDB, Messaging, Storage, Hosting, Functions) and React Antd UI components for CMS.\n\n\nQIWI Match-IT application\nThis app was developed by my company. My job was Crossplatform social application for IT-conference with Tinder-ish swipe mechanics and online chat. Made it with React.Native\n\n\nBackend and frontend for online fashion magazine site and application\nThis app and this site were developed by my company. My job was creating CMS (Wagtail) and API (Rest Framework) for application as well as developing the site (django + Vue.js)\n\n\nPractica Publishers online store\nHere is one of my first works - eCommerce site made with oscar-django in 2014"
  },
  {
    "objectID": "projects-list.html#startups-personal-projects",
    "href": "projects-list.html#startups-personal-projects",
    "title": "Projects List",
    "section": "Startups / Personal projects",
    "text": "Startups / Personal projects\n\nIOS Health App\nIn 2021 I joined an interesting startup that was developing an IOS health app. I did a lot of RnD work around Apple Watch and IOS health related data. Also I developed an architecture to support secure decentralized sharing of health data between the patient and a their doctors.\nIOS and Watch apps were done in SwiftUI. The backend architecture was based on NodeJS and Kafka.\n\n\nCalke\ncalke.ge is a waste separation and collection startup.\nAll business part is made with ODOO framework (Python). I’m providing several front applications (React.Native) as well as general technical supervision.\n\n\nGamo\ngamoapp.com is a car polling startup in Tbilis. Hopefully soon there will be MVP opening.\nRecently I joined this wonderfull team as a fullstack developer. I had to finish up multiple features in their app (React.Native) as well as create new api endpoints (Express/MongoDB). I also created couple pages for Admin web app (React)."
  },
  {
    "objectID": "projects-list.html#other-projects",
    "href": "projects-list.html#other-projects",
    "title": "Projects List",
    "section": "Other Projects",
    "text": "Other Projects\n\nMultiple crypto-currency facilities\nFullstack WebMoney - Crypto exchange platform made on Django/Vue\nCrypto Assets Portfolio control-panel, that allows too implement united trade strategy accross multiple accounts on multiple trade platforms (Kraken/Binance/Poloniex)\nAlgorythmic trading solution based on python Catalyst framework\n\n\nAnalytics System for restaurants\nMultiple charts made with Highcharts/React Backend made with Django/Python collecting online restaurant business data from RK7 servers\n\n\nAnalytics System for big pharma holding\nIn 2021 I worked solo on developing an analytical system based on Odoo ERP platform. The system was injesting data from multiple companies presenting it in several dashboards and generating reports."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Behind My Code",
    "section": "",
    "text": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI\n\n\nPart 4: Final\n\n\n\n\ncode\n\n\nswift\n\n\nweb\n\n\nai\n\n\n\n\n\n\n\n\n\n\n\nSep 2, 2023\n\n\nArtem Putilov\n\n\n\n\n\n\n  \n\n\n\n\nLearning Swift Through Building MFEncoder: A Journey Guided by OpenAI\n\n\nPart 3: Diving deep into Encoder\n\n\n\n\ncode\n\n\nswift\n\n\nweb\n\n\nai\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nArtem Putilov\n\n\n\n\n\n\n  \n\n\n\n\nLearning Swift Through Building MFEncoder: A Journey Guided by OpenAI\n\n\nPart 2: Implementing FormData\n\n\n\n\ncode\n\n\nswift\n\n\nweb\n\n\nai\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nArtem Putilov\n\n\n\n\n\n\n  \n\n\n\n\nLearning Swift Through Building MFEncoder: A Journey Guided by OpenAI\n\n\nHow We Used OpenAI’s GPT-4 as a Virtual Mentor in Swift Development\n\n\n\n\ncode\n\n\nswift\n\n\nweb\n\n\nai\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2023\n\n\nArtem Putilov\n\n\n\n\n\n\n  \n\n\n\n\nSolving random LeetCode problems\n\n\nJumps Game\n\n\n\n\npython\n\n\nalgo\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2023\n\n\nArtem Putilov\n\n\n\n\n\n\n  \n\n\n\n\nMobile App Idea: Language Learning App\n\n\n\n\n\n\n\nai\n\n\nios\n\n\napi\n\n\nideas\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nArtem Putilov\n\n\n\n\n\n\n  \n\n\n\n\nMicrofrontends with Holodeck Inc\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nmicrofrontends\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2023\n\n\nArtem Putilov\n\n\n\n\n\n\n  \n\n\n\n\nHolodeck Inc. Filter Builder\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nholodeck\n\n\nelasticsearch\n\n\nreact\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\nArtem Putilov\n\n\n\n\n\n\n  \n\n\n\n\nHolodeck Inc. Elastic Queries\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\nArtem Putilov\n\n\n\n\n\n\n  \n\n\n\n\nFake Innovation\n\n\n\n\n\n\n\ninitial_commit\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2022\n\n\nArtem Putilov\n\n\n\n\n\n\nNo matching items\n\n  Artem Putilov, 2023"
  },
  {
    "objectID": "posts/algo-jumps/algo-jumps.html",
    "href": "posts/algo-jumps/algo-jumps.html",
    "title": "Solving random LeetCode problems",
    "section": "",
    "text": "Given an array of integers arr, you are initially positioned at the first index of the array.\nIn one step you can jump from index i to index:\ni + 1 where: i + 1 &lt; arr.length. i - 1 where: i - 1 &gt;= 0. j where: arr[i] == arr[j] and i != j. Return the minimum number of steps to reach the last index of the array.\nNotice that you can not jump outside of the array at any time.\nExample 1:\nInput: arr = [100,-23,-23,404,100,23,23,23,3,404] Output: 3 Explanation: You need three jumps from index 0 –&gt; 4 –&gt; 3 –&gt; 9. Note that index 9 is the last index of the array.\n\n\nBFS seems like a straghtforward way to do it since we are searching for the shortest way in the graph. We will require: - a function that pickes next steps based on the roles (back, forward, jump) - for picking where to jump we will use a mapping from values into their positions int -&gt; [int] - a simple que of next steps - in order to avoid cycles we need another hash of visited positions we will also use it to retrace our steps\n\nfrom typing import List, Dict, Set, Deque\nfrom collections import deque\n\n\nclass Solution:\n\n    m: Dict[int, List[int]]\n    q: Deque[int]\n    distances: Dict[int, int]\n    arr: List[int]\n\n    def __init__(self):\n        self.m = dict()\n        self.q = deque()\n        self.distances = dict()\n\n    def minJumps(self, arr: List[int]) -&gt; int:\n        # edge cases\n        if len(arr) &lt; 3:\n            return len(arr) - 1\n        self.arr = arr\n        self.buildM()\n        result = self.pickNext(0)\n        while len(self.q) &gt; 0:\n            if result is not None:\n                break\n            position = self.q.popleft()\n            result = self.pickNext(position)\n        return result if result is not None else 0\n\n    def compressInput(self, arr: List[int]) -&gt; List[int]:\n        result = [arr[0]]\n        for i in range(1, len(arr)-1):\n            if arr[i] != arr[i-1] or arr[i] != arr[i+1]:\n                result.append(arr[i])\n        result.append(arr[-1])\n        return result\n\n    def buildM(self):\n        for (i, x) in enumerate(self.arr):\n            ml = self.m.setdefault(x, [])\n            ml.append(i)\n\n    def pickNext(self, pos: int) -&gt; None | int:\n\n        ml = reversed(self.m.get(self.arr[pos], []))\n        self.m[self.arr[pos]] = []\n        distance = self.distances.get(pos,0)\n\n        nearest = []\n        if pos &lt; len(self.arr) - 1:\n            nearest.append(pos + 1)\n        if pos &gt; 0:\n            nearest.append(pos - 1)\n\n        for t in [ml, nearest]:\n            for p in t:\n                if p not in self.distances:\n                    if p &gt;= len(self.arr) - 1:\n                        return distance + len(self.arr) - p \n                    self.q.append(p)\n                    self.distances[p] = distance + 1\nArtem Putilov, 2023"
  },
  {
    "objectID": "posts/algo-jumps/algo-jumps.html#bfs-solution",
    "href": "posts/algo-jumps/algo-jumps.html#bfs-solution",
    "title": "Solving random LeetCode problems",
    "section": "",
    "text": "BFS seems like a straghtforward way to do it since we are searching for the shortest way in the graph. We will require: - a function that pickes next steps based on the roles (back, forward, jump) - for picking where to jump we will use a mapping from values into their positions int -&gt; [int] - a simple que of next steps - in order to avoid cycles we need another hash of visited positions we will also use it to retrace our steps\n\nfrom typing import List, Dict, Set, Deque\nfrom collections import deque\n\n\nclass Solution:\n\n    m: Dict[int, List[int]]\n    q: Deque[int]\n    distances: Dict[int, int]\n    arr: List[int]\n\n    def __init__(self):\n        self.m = dict()\n        self.q = deque()\n        self.distances = dict()\n\n    def minJumps(self, arr: List[int]) -&gt; int:\n        # edge cases\n        if len(arr) &lt; 3:\n            return len(arr) - 1\n        self.arr = arr\n        self.buildM()\n        result = self.pickNext(0)\n        while len(self.q) &gt; 0:\n            if result is not None:\n                break\n            position = self.q.popleft()\n            result = self.pickNext(position)\n        return result if result is not None else 0\n\n    def compressInput(self, arr: List[int]) -&gt; List[int]:\n        result = [arr[0]]\n        for i in range(1, len(arr)-1):\n            if arr[i] != arr[i-1] or arr[i] != arr[i+1]:\n                result.append(arr[i])\n        result.append(arr[-1])\n        return result\n\n    def buildM(self):\n        for (i, x) in enumerate(self.arr):\n            ml = self.m.setdefault(x, [])\n            ml.append(i)\n\n    def pickNext(self, pos: int) -&gt; None | int:\n\n        ml = reversed(self.m.get(self.arr[pos], []))\n        self.m[self.arr[pos]] = []\n        distance = self.distances.get(pos,0)\n\n        nearest = []\n        if pos &lt; len(self.arr) - 1:\n            nearest.append(pos + 1)\n        if pos &gt; 0:\n            nearest.append(pos - 1)\n\n        for t in [ml, nearest]:\n            for p in t:\n                if p not in self.distances:\n                    if p &gt;= len(self.arr) - 1:\n                        return distance + len(self.arr) - p \n                    self.q.append(p)\n                    self.distances[p] = distance + 1"
  },
  {
    "objectID": "posts/holodeck-microfrontends/index.html",
    "href": "posts/holodeck-microfrontends/index.html",
    "title": "Microfrontends with Holodeck Inc",
    "section": "",
    "text": "BURLEIGH: Lucy. Thank God you’ve come back. Why are you dressed so strangely? JANEWAY: It’s a costume. BURLEIGH: You’d look lovely in anything. I’ve thought of you constantly. Remembered your touch, your perfume, your lips JANEWAY: Computer, delete character.\n\n(Startrek Voyager: Persistence of Vision)\nWe are the employees of Holodeck Inc, who develops and maintains the system, working on latest updates and improvements.\nOur platform is responsible for supplying the holodecks all around the world with numerous new hologram characters, finding replacements, picking the best candidates for the holo-plots, advertisements and etc.\nThrough our platform the holo-producers can reach to holo-creators and we are provide the necessary tools for them to share and distribute their content (latter) search for content and compile it into the final product (formers).\nOur platform consists of 2 mobile apps - one for holo-producers and another for holo-artists and a central dashboard that allows our admins and holo-casters to assist producers in finding everything they need.\n\n\n\nPlatform\n\n\nRecently Holodeck Inc management decided to completely revamp their central dashboard and we were tasked to create a new and shiny web console made with ReactJS. One of the major new features there is our brand new Holoalbum - a dynamic and powerful react.js component that lets our holo-casters prepare a list of recommended holo-characters with all their characteristics, multiple photos and etc. This component lets manually order, filter, hide and further fine tune the list. The result should be visible within the ProdApp as a similar but much simplified version.\n\n\n\nHoloAlbum\n\n\nThe most challenging here was that we had to implement Holoalbum twice: admin version on a new Fresh react.js dashboard webapp and limited producers version on HoloProd app. HoloProd app was initially developed by subcontractors as react.native app for iPads. Unfortunately implementation was very poor. Later we had to add a lot of functionality there including web version (react native web), mfa (Firebase) and secure encrypted offline storage that was leveraging service workers in web version. Since it was a project dedicated to revamping admin dashboard and not the apps and we were very limited in terms of time and resources we would prefer to deal with HoloProd codebase as little as possible.\nAfter serious considerations we decided to try a new micro frontend architecture to achieve maximal code reusability and to minimize the unpleasant updates to react.native app code.\nHolodeck Inc management agreed Holoalbums to be a web version only feature (iPad version was very rarely used by the time anyway ).\n\nFirst Approach: Simply Copy the Code\nWe had a ready-made Holoalbum component that we used in the admin panel. We needed to use it in HoloProd. But HoloProd was written in react.native, and we didn’t want to touch it. Therefore, we decided to create a new CRA project for the producer version of the Holo Album, transferring the main album components from the admin panel and connecting it to HoloProd as a micro frontend.\n\n\n\nInitial Composition\n\n\nWe quickly tested this scenario on an empty component and confirmed that the idea was workable and that we could indeed just “connect” a new component for viewing albums to the web version of the react.native mobile app.\nThe web version of the app was built using Webpack, which we modified by adding a new plugin.\n    new ModuleFederationPlugin({\n    name: \"host\",\n    remotes: {\n        remote: `remote@${PRODUCER_ALBUM_APP_URL}/remoteEntry.js`,\n    },\n    shared: {\n        react: {\n        singleton: true,\n        requiredVersion: packageJson.dependencies[\"react\"],\n        },\n        \"react-dom\": {\n        singleton: true,\n        requiredVersion: packageJson.dependencies[\"react-dom\"],\n        },\n    },\n    })\nAlmost everything here is taken from the simplest example in the Module Federation repository.\nThe variable PRODUCER_ALBUM_APP_URL contains the address of the web version of HoloProd. We pass it to the script through environment variables.\nconst PRODUCER_ALBUM_APP_URL =\n  process.env.PRODUCER_ALBUM_APP_URL || \"http://localhost:3002\";\nThe documentation’s description of shared modules is quite convoluted: &gt; Shared modules are modules that are both overridable and provided as overrides to nested containers. They usually point to the same module in each build, e.g., the same library.\n\nThe packageName option allows setting a package name to look for a requiredVersion. It is automatically inferred for the module requests by default, set requiredVersion to false when automatic inference should be disabled.\n\nIn practice, this means that you need to specify in shared modules those dependencies that can be used in both projects and specify specific versions of these dependencies unless they match. You don’t need to list all modules, as done in the example. In our case, doing so led to compilation errors because react.native modules have build peculiarities. Additionally, the project had links to custom modules in GitHub, which also caused errors.\nBesides that, we immediately encountered a compilation error, which is the first one listed in the documentation:\n\nUncaught Error: Shared module is not available for eager consumption\n\nAs the documentation suggests, the issue is the attempt to import modules that load asynchronously. So, if in App.js, for example, we import react, and this module still needs to be downloaded asynchronously, the application won’t be able to find it. Therefore, the documentation recommends using lazy loading. Move the entry file’s code (in our case index.web.js) to a separate module (bootstrap.web.js), and in the main entry file, leave only the asynchronous import.\n// index.web.js\nimport(\"./bootstrap.web\");\n\nIntegrating the Component into Navigation\nAccess to a specific album is made via a unique link that we obtain from the admin panel. There’s no general list of albums; the page of a specific album retrieves the ID from the link and requests the album’s data.\nIn the react-navigation-based navigation, we added a new screen that displays the album page. For this, in the homeNavigation.js file, we incorporated a new screen into the navigation stack:\nconst HomeNavigation = createDrawerNavigator(\n  {\n    /*...*/\n    Lookbooks: {\n      screen: ProdAlbum,\n    },\n  },\nAs evident from the code, we utilize an Android-like navigation with a side menu. During testing, we tried adding the screen to other navigation stacks, but eventually settled on this option.\nProdAlbum.js has two versions, differentiated by a suffix following the RN convention: ProdAlbum.web.js and ProdAlbum.ios.js. The latter file is used to display an empty screen in the iOS app where, per the task requirements, Albums are not supported. The web version, ProdAlbum.web.js, checks user authentication and integrates our micro-frontend, which presents the album page.\n// ProdAlbum.web.js\n\n  if (!token) {\n    return &lt;Text&gt;Loading...&lt;/Text&gt;;\n  }\n  return (\n    &lt;React.Suspense fallback={&lt;Text&gt;Loading...&lt;/Text&gt;}&gt;\n      &lt;Header style={[styles.headerStyle]} transparent&gt;\n        &lt;StatusBar barStyle=\"light-content\" /&gt;\n        &lt;Left&gt;\n          &lt;HeaderLogoButton onPress={() =&gt; props.navigation.toggleDrawer()} /&gt;\n        &lt;/Left&gt;\n        &lt;Body style={{ ...Platform.select({ web: { position: \"absolute\" } }) }}&gt;\n          &lt;Text style={styles.titleStyle}&gt;Lookbooks&lt;/Text&gt;\n        &lt;/Body&gt;\n        &lt;Right /&gt;\n      &lt;/Header&gt;\n      &lt;ScrollView&gt;\n        &lt;RemoteProducerAlbum idToken={token} /&gt;\n      &lt;/ScrollView&gt;\n    &lt;/React.Suspense&gt;\n  )\n\n\nUsing Micro-Frontends with React and Webpack\nWe utilize React.Suspense to display a loading spinner while the micro-frontend is being loaded. Moreover, we employ the standard screen framework: a Header combined with a ScrollView to present our external component within the app’s default screen. To ensure our micro-frontend can successfully make API requests, we pass it the user’s token, which we obtain during authentication.\nThe external component is loaded using React.lazy, allowing for dynamic component loading. In our context, since we use Webpack to bundle our application, we can employ the import() directive for component loading.\n// ProdAlbum.web.js\nconst RemoteProducerAlbum = React.lazy(() =&gt; import(\"remote/App\"));\nremote/App.js serves as the entry point to our micro-frontend. Within this file, we export our Album component to be displayed inside our host application.\nThe modifications to the host application are limited to:\n\nCrafting a screen framework that displays the external component.\nIncorporating it into the navigation.\nAdapting the Webpack configuration to integrate the Module Federation Plugin.\n\nThis approach equipped us to address the following challenges:\n\nMinimized Changes to Host Application: As observed, the only significant modification required was integrating the Module Federation Plugin. Everything else involved creating new code or making minor adjustments to the existing code.\nLeveraging Host Application Capabilities: Album access utilizes the same security mechanisms (MFA) as within the host application. Additionally, we can capitalize on any other features of the host application, like employing Redux for data storage accessible within the external component, or utilizing built-in navigation and styling components.\n\n\n\n\nSecond Approach: Utilizing the Admin’s Codebase\nSoon, we realized that we could simply use the admin’s codebase to develop a new producer web application. Instead of extracting Album components from the admin panel, we created a separate container wrapper that internally used the same components as the main album container. The differences were the unavailability of some editing tools, hiding a few extra elements, and the use of a specialized service fetching the Album’s data from a different endpoint. Unlike the admin endpoint, this one accepted producer tokens and performed additional access checks.\n\n\n\nFinal Composition\n\n\nThis way, we eliminated the need for an extra application and redundant code. To launch and build the producer web app, we added separate scripts in the package.json, drawing inspiration from the same example by the plugin’s author. Since the admin panel was initially built on CRA (Create React App), this example was highly beneficial for us.\nFollowing the example, we exported the plugin configuration to a separate file named modulefederation.config.js. Everything here is standard: it’s essential that the container’s name remained the same as we used in the host application (remote), and the path to the root file matched what we use in the host app’s import (App).\nmodule.exports = {\n  name: 'remote',\n  exposes: {\n    './App': './src/App',\n  },\n  filename: 'remoteEntry.js',\n  /* ... */\n}\nUsing separate scripts for launching and building allows us to modify the default webpack configuration, incorporating the ModuleFederationPlugin and feeding it settings from modulefederation.config.js. In doing so, we don’t have to introduce any specialized tools like rewired. Everything remains transparent and straightforward.\nBesides integrating the plugin, our webpack.config override redefines the entry point: rather than index.tsx, which initializes the full-blown admin app, we designate a specific file that solely initializes our ProdAlbum component.\n// override/webpack.config.js\nconst override = config =&gt; {\n  config.plugins.push(new ModuleFederationPlugin(require('../../../modulefederation.config.js')));\n\n  config.output.publicPath = 'auto';\n  config.entry = [\n    \"./src/index-producers-lookbook\"\n  ]\n\n  return config;\n};\nThis index employs the same technique for asynchronous component loading: directly in the index file, there’s only one asynchronous import that fetches the bootstrap module. This module, in turn, imports and passes to the ReactDOM.render a wrapper application, which returns the album component and its context.\n// ProdAlbumApp.tsx\n\nconst ProdAlbumApp = (props: { idToken: string }) =&gt; {\n    /* ... */\n    const prodAlbumPorts = useMemo(\n        () =&gt; ProdAlbumServiceFactory.getPortsContext(props.idToken, toastsVM, loadingIndicatorVM),\n        [props.idToken]\n    );\n    const [documentId, setDocumentId] = useState('');\n    useEffect(() =&gt; {\n        const id = new URL(window.location.href).searchParams.get('id');\n        if (id) {\n            setDocumentId(id);\n        }\n    }, []);\n    if (!documentId) {\n        return &lt;Spinner /&gt;;\n    }\n    return (\n        &lt;CastingPortsContext.Provider value={prodAlbumPorts}&gt;\n            &lt;div className='bg-white p-2'&gt;\n                &lt;ProdAlbumDocumentContainer\n                    documentId={documentId}\n                    documentState={DocumentStateEnum.Edit}\n                /&gt;\n            &lt;/div&gt;\n        &lt;/CastingPortsContext.Provider&gt;\n    );\n};\nA significant example where we utilize the document ID, extracting it from the URL, as well as the idToken passed to the component, which we obtained in the host application. This token is used to initialize a specific service that fetches Album data from a particular endpoint, verifying a specific producer’s access rights to this document.\nThis way, we can avoid concerns about initializing all other services and contexts used in the admin panel (authorization, global redux store, etc.), and connect directly to our component, preparing only the necessary contexts and services for it.\nWorthy of separate mention is the publicPath parameter set in the example to ‘auto’. As per the documentation, it defines the base path for all static resources that will be generated by Webpack. In the CRA template, this parameter is based on PUBLIC_URL or homePage in package.json. If we want to use the same template for different applications but with different publicPaths, we can use the ‘auto’ value, allowing Webpack to independently determine the publicPath based on the current URL. Since we use homePage in the main project, we tried to set this same path in publicPath. However, this didn’t work, and we encountered a loading error:\n\nUncaught ChunkLoadError: Loading chunk … failed.\n\nThe reason being, since we compile a separate Album bundle and place it on a separate hosting, the homePage in package.json will not match the current URL. In the end, we settled on the following solution: keep the value of publicPath = ‘auto’, and directly in the launch and build scripts, rewrite the PUBLIC_URL to its default value:\nprocess.env.PUBLIC_URL = '/'\n\n\nDeployment\nThe build and publication of the web version of ProdApp and the ProdAlbum bundle are executed using Firebase’s static hosting and Github Actions CI. Firebase offers all the conveniences, including workflow templates for Github Actions, as well as automated addresses for preview versions. We store the path to the ProdAlbum bundle (PRODUCER_ALBUM_APP_URL) in GitHub secrets and remember to pass it in the corresponding workflow.\n    # Prod App workflow\n    # ...\n    - name: Build\n    run: yarn run web:build-stage\n    env:\n        PRODUCER_ALBUM_APP_URL: ${{ secrets.PRODUCER_ALBUM_APP_URL }}\nIn both cases, for the SPA to work correctly, it is necessary to set up redirection rules to index.html. In Firebase, this is done using the firebase.json file:\n{\n  \"hosting\": {\n    \"rewrites\": [\n      {\n        \"source\": \"**\",\n        \"destination\": \"/index.html\"\n      }\n    ]\n  }\n}\nAdditionally, to display the fonts correctly from the ProdAlbum bundle, it was necessary to add the Access-Control-Allow-Origin header:\n{\n\"headers\": [ {\n    \"source\": \"**/*.@(eot|otf|ttf|ttc|woff|font.css)\",\n    \"headers\": [ {\n        \"key\": \"Access-Control-Allow-Origin\",\n        \"value\": \"*\"\n    } ]\n} ]\n\n}\n\n\nConclusions\nWe believe that this case serves as an example of effective microfrontends implementation in practice. We managed to significantly reduce development time by “plugging in” new features into an outdated project, taking it from a codebase that was actively being developed and almost ready to use, rather than rebuilding it on the base of an older project. Additionally, we avoided duplicating code that was common to both projects, allowing its use in both applications.\nOf course, this led to the emergence of an additional dependency: ProdApp now relies on the ProdAlbum bundle, which is part of the Dashboard codebase. However, since the ProdAlbum is built and deployed separately from the Dashboard, this dependency doesn’t introduce any additional risks that an update to the Dashboard might inadvertently break the ProdApp. Moreover, this dependency adheres to the “single responsibility principle,” since the business logic of the Album is concentrated in one place. If anything changes regarding how the Albums look or operate in the Dashboard, it would most likely result in changes to ProdAlbum.\nA key point in this case in favor of choosing microfrontends was how easily we were able to transfer the authorization token from the ProdApp environment to ProdAlbum. Had we simply launched a microsite for viewing the Albums and placed it on the same domain as the ProdApp, we would not have been able to leverage the authorization in ProdApp for access to ProdAlbum. Instead, we would have had to implement another authentication mechanism for ProdAlbum, which would not have been optimal from a security standpoint and would have required additional user actions.\nWe’d like to thank the plugin creator, Zack Jackson, for his work and the excellent set of examples from which we drew a lot of valuable insights (not only for this case). On the other hand, we feel that the documentation for the plugin and Webpack in general could be more detailed and clear. For instance, the explanations regarding shared modules and publicPath in the plugin’s documentation are not entirely clear and don’t provide a full understanding of how they operate.\n\n\n\n\n  Artem Putilov, 2023"
  },
  {
    "objectID": "posts/mfencoder/part_3.html",
    "href": "posts/mfencoder/part_3.html",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "",
    "text": "In this set of articles, we take a close look at Swift programming. We’re focusing on creating a package called “MFEncoder” to handle multipart form data. This is important for things like sending forms and files over the internet. We will use the OpenAI GPT-4 chatbot as a guide or mentor to help us through the project.\n\n\n\nIn Part 1 we discussed the details of Multipart form encoding. We also built some foundational elements as well as interactive Playground to help us with testing our implementation with the real backend.\nIn Part 2 we implemented MFFormData - a low-level API, inspired by the Web’s FormData API, that gives complete control over the form data before submitting it over HTTP.\nArtem Putilov, 2023"
  },
  {
    "objectID": "posts/mfencoder/part_3.html#brief-summary",
    "href": "posts/mfencoder/part_3.html#brief-summary",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "",
    "text": "In this set of articles, we take a close look at Swift programming. We’re focusing on creating a package called “MFEncoder” to handle multipart form data. This is important for things like sending forms and files over the internet. We will use the OpenAI GPT-4 chatbot as a guide or mentor to help us through the project."
  },
  {
    "objectID": "posts/mfencoder/part_3.html#previous-parts",
    "href": "posts/mfencoder/part_3.html#previous-parts",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "",
    "text": "In Part 1 we discussed the details of Multipart form encoding. We also built some foundational elements as well as interactive Playground to help us with testing our implementation with the real backend.\nIn Part 2 we implemented MFFormData - a low-level API, inspired by the Web’s FormData API, that gives complete control over the form data before submitting it over HTTP."
  },
  {
    "objectID": "posts/mfencoder/part_3.html#about-encoder-and-encodable-protocols",
    "href": "posts/mfencoder/part_3.html#about-encoder-and-encodable-protocols",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "About Encoder and Encodable protocols",
    "text": "About Encoder and Encodable protocols\nWhy do we need to implement MFEncoder, isn’t MFFormData enough?\nThe Encoder and Encodable protocols in Swift are designed in a declarative manner. This focus allows us to specify what needs to be done, rather than detailing how to do it. By defining what properties our data structure has and marking it as Encodable, the compiler and runtime take care of the rest. This design offers several advantages:\n\nSimplifies Our Code\nBecause we’re only declaring what to encode, we don’t need to write imperative code to convert each property to a transportable data format. The Encoder itself knows how to take our declaratively defined structure and serialize it, significantly simplifying our codebase.\n\n\nEncourages Consistency\nA declarative approach encourages a consistent method of data encoding and decoding across our entire project or even across multiple projects. Every type that conforms to Encodable can be serialized in the same way, using the same Encoder types.\n\n\nEnhances Maintainability\nSince our code is not concerned with the ‘how,’ it becomes easier to maintain. If Swift or its libraries update the underlying encoding mechanisms, our Encodable types should continue to work without modification.\n\n\nEnables Abstraction\nThe Encoder protocol abstracts the details of the encoding storage and format. Because these details are abstracted away, we can switch between different encoding formats (like JSON, Plist’s, Multipart Form etc.) without having to change the model classes/structures themselves.\n\n\nFacilitates Extensibility\nSwift’s Encodable protocol makes it straightforward to add encoding capabilities to existing types through extensions. If we have a type that we wish to encode but don’t control its source code, we can extend it to conform to Encodable, assuming its properties are also Encodable.\n\n\nExample\nstruct Book: Encodable {\n    let title: String\n    let author: String\n    let publishDate: Date\n}\n\nlet ourBook = Book(title: \"1984\", author: \"George Orwell\", publishDate: Date())\nIn this example, just by declaring Book as Encodable, we’ve established that a book can be encoded to any format supported by an Encoder. We don’t need to specify how this encoding happens; it’s all managed by the protocol and the specific Encoder instance we choose to use (like JSONEncoder). This is the essence of declarative programming."
  },
  {
    "objectID": "posts/mfencoder/part_3.html#approaching-implementation",
    "href": "posts/mfencoder/part_3.html#approaching-implementation",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "Approaching implementation",
    "text": "Approaching implementation\nImplementing a custom encoder is an advanced task that requires a good understanding of the encoding process and how Swift handles different types of data. GPT-4 provides us with a very high level skeleton and advises to look into JSONEncoder implementation which can be found in the Swift project on GitHub\nHere’s what GPT-4 gave us an example of how we might start implementing a custom encoder. This example doesn’t fully implement an encoder, but it does show the structure we’d need to follow.\npublic struct CustomEncoder: Encoder {\n    public var codingPath: [CodingKey] = []\n    public var userInfo: [CodingUserInfoKey: Any] = [:]\n\n    public func container&lt;Key&gt;(keyedBy type: Key.Type) -&gt; KeyedEncodingContainer&lt;Key&gt; where Key: CodingKey {\n        let container = CustomKeyedEncodingContainer&lt;Key&gt;(codingPath: codingPath, userInfo: userInfo)\n        return KeyedEncodingContainer(container)\n    }\n\n    public func unkeyedContainer() -&gt; UnkeyedEncodingContainer {\n        CustomUnkeyedEncodingContainer(codingPath: codingPath, userInfo: userInfo)\n    }\n\n    public func singleValueContainer() -&gt; SingleValueEncodingContainer {\n        CustomSingleValueEncodingContainer(codingPath: codingPath, userInfo: userInfo)\n    }\n}\nNotice how Encoder doesn’t even require the encode method. CustomKeyedEncodingContainer, CustomUnkeyedEncodingContainer, and CustomSingleValueEncodingContainer are placeholder types and we need to implement these types to properly handle the encoding.\nEach of these methods is part of the Encoder protocol and is used when encoding different types of data. The actual work of encoding the data will be performed by the CustomKeyedEncodingContainer, CustomUnkeyedEncodingContainer, and CustomSingleValueEncodingContainer types."
  },
  {
    "objectID": "posts/mfencoder/part_3.html#learning-from-jsonencoder-source-code",
    "href": "posts/mfencoder/part_3.html#learning-from-jsonencoder-source-code",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "Learning from JSONEncoder source code",
    "text": "Learning from JSONEncoder source code\nLets follow the advice and try to figure out some details of encoding in Swift by looking at JSONEncoder implementation\nWe will describe central entities, their roles, relationships and some of their methods. We will also provide a sequence chart of encoding process.\n\nJSONValue\nFundamental data structure that is used to represent full JSON payload within Swift types system. Its a recursive polymorphic enum, containing 4 basic JSON types:\n\nstring\nnumber\nboolean\nnull\n\nas well as 2 container types:\n\nlist, represented as an array of JSONValues\nobject, represented as a dictionary [String: JSONValue]\n\nJSONValue is what gets assembled as a result of all encoding operations. Having a top-level JSONValue takes a trivial recursive operation to produce the wanted JSON string.\n\n\nJSONFuture\nWraps JSONValue making possible to reference its nested containers and update them during serialization on all levels. To achieve this JSONFuture holds reference based wrappers around primitive array and dictionary.\nprivate enum JSONFuture {\n    case value(JSONValue)\n    case nestedArray(RefArray)\n    case nestedObject(RefObject)\n\n    class RefArray {\n        private(set) var array: [JSONFuture] = []\n        // ... multiple mutate methods omitted ...\n    }\n\n    class RefObject {\n        private(set) var dict: [String: JSONFuture] = [:]\n        // ... multiple mutate methods omitted ...\n    }\n}\nWhen serializing complex, nested structures like JSON objects — which can include other objects, or arrays containing additional arrays — we encounter a specific challenge related to Swift’s value semantics. In a nested structure, any modification to a deep-level object would only affect a copy of that object, leaving the overall parent structure unchanged. This limitation makes a basic JSONValue approach insufficient for our purposes, prompting the need for reference types like RefArray and RefObject during the encoding process.\nBoth RefArray and RefObject have the capability to create each other, nesting new reference containers within themselves. Additionally, they offer value getters that output them as plain dictionaries or arrays containing JSONValues. These getters enable the recursive assembly of the complete payload into a JSONValue tree, which is essential for the final serialization step.\nThese reference wrappers are used by Keyed and Unkeyed under the hood.\n\n\nKeyed, Unkeyed and Single value containers\nContainers do the actual encoding work. Each includes multiple overloaded encode methods to deal with all possible primitive values in Swift as well as any user types corresponding to Encodable protocol. Each wraps its kind of container:\n\nJSONFuture.RefObject for Keyed\nJSONFuture.RefArray for Unkeyed\nJSONValue for SingleValue\n\nKeyed and Unkeyed can create and return nested containers, building the tree of JSONFuture polymorphic entities.\n\n\nJSONEncoderImpl\nImplements Encoder protocol, holding the root (or sub root) node as one of 3 possible JSONFuture values from above. Implements container generation methods to comply to protocol. Holds current path which can be accessed by user encode methods.\nEach container holds reference to its parent encoder implementation.\n\n\n_SpecialTreatmentEncoder\nIs internal protocol that is used like kind of Template Class for both JSONEncoderImpl and all Container classes. It holds default implementation for several convenient methods used by both Encoder and Containers.\n\nwrapDate - uses DateEncodingStrategy to provide different convenient methods for date encoding\nsimilar wrappers for Data and Object (Dictionary)\nwrapEncodable&lt;E: Encodable&gt; - is the central encoding dispatch. For complex Swift objects like Date, Data, Dict it calls internal wrapping methods. For user types, it generates a new JSONEncoderImpl instance and initiates a recursive encoding. It produces JSONValue for final serialization.\n\nThis is the relationship scheme of described objects. \n\n\nJSONEncoder\nThe main class is a Facade form JSONEncoderImpl, that stores the options, like dateEncodingStrategy or field name conversion strategy etc. It provides main top level entrypoint - encode method:\n    open func encode&lt;T: Encodable&gt;(_ value: T) throws -&gt; Data {\n        let encoder = JSONEncoderImpl(options: self.options, codingPath: [])\n        let value: JSONValue = try encoder.wrapEncodable(value, for: nil)\n        let writer = JSONValue.Writer(options: self.outputFormatting)\n        let bytes = writer.writeValue(value)\n\n        return Data(bytes)\n    }"
  },
  {
    "objectID": "posts/mfencoder/part_3.html#encoding-process-example",
    "href": "posts/mfencoder/part_3.html#encoding-process-example",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "Encoding process example",
    "text": "Encoding process example\nHere is an example of advanced Encoder usage in client code\nstruct ContactInfo: Encodable {\n\nvar firstName: String = \"John\"\nvar lastName: String = \"Smith\"\n\nvar homePhone: String = \"11 222 3333\"\nvar cellularPhone: String = \"+11 222 333 444\"\nvar email: String = \"john@email.com\"\n\nlet website: String = \"https://example.com\"\nlet linkedIn: String = \"https://linkedin.com/john\"\nlet github: String = \"https://github.com/john\"\n\nfunc encode(to encoder: Encoder) throws {\n    var contact = encoder.container(keyedBy: ContactInfoKeys.self)\n    try contact.encode(\"\\(firstName) \\(lastName)\", forKey: .name)\n    try contact.encode(email, forKey: .email)\n    \n    var phones = contact.nestedUnkeyedContainer(forKey: .phones)\n    try phones.encode(homePhone)\n    try phones.encode(cellularPhone)\n    \n    var sites = contact.nestedContainer(keyedBy: SitesKeys.self, forKey: .sites)\n    print(\"Contact Info coding path: \\(encoder.codingPath)\")\n    print(\"Sites coding path: \\(sites.codingPath)\")\n    try sites.encode(website, forKey: .personal)\n    try sites.encode(linkedIn, forKey: .linkedIn)\n    try sites.encode(github, forKey: .github)\n}\n\n}\n\nlet encoder = JSONEncoder()\nencoder.outputFormatting = [.prettyPrinted, .withoutEscapingSlashes, .sortedKeys]\n\nlet data = try encoder.encode(ContactInfo())\nIn this example, we have a ContactInfo struct that conforms to the Encodable protocol.\n\nCustom Encoding Logic\nInstead of relying on the default behavior of the Encodable protocol, we implement the encode(to:) method to manually dictate how the data should be serialized into JSON.\nHere are the key points:\n\nCombining Fields: The firstName and lastName are combined and encoded as a single JSON key-value pair with the key “name”.\nNested Unkeyed Container for Phones: Both homePhone and cellularPhone are packed into an unkeyed container (essentially an array) under the key “phones”.\nNested Keyed Container for Sites: The various website URLs (website, linkedIn, github) are grouped into a nested keyed container under the key “sites”.\nDebugging with Coding Paths: The coding path of each nested container is printed, helping us to debug or understand the hierarchy of our encoding.\n\n\n\nEncoder Configuration\nWe use a JSONEncoder to serialize our ContactInfo object into JSON format. The encoder is configured to produce pretty-printed JSON, without escaping slashes, and with sorted keys.\n\n\nEncoding process sequence\nNow lets look at the sequence chart of this process\n\n\n\n\nsequenceDiagram\n    actor User\n\n    User-&gt;&gt;JSONEncoder: encode Encodable\n    JSONEncoder-&gt;&gt;JSONEncoderImpl: wrap Encodable\n    JSONEncoderImpl-&gt;&gt;Encodable: encode to Self\n\n    Encodable-&gt;&gt;JSONEncoderImpl: ask for Container\n    JSONEncoderImpl-&gt;&gt;Encodable: generate and return Container\n    loop for each field \n        Encodable-&gt;&gt;Container: ask for nested Container\n        Container-&gt;&gt;JSONFuture: generate nested JSONFuture object\n        JSONFuture-&gt;&gt;Container: return nested JSONFuture object\n        Container-&gt;&gt;Encodable: generate and return nested Container\n        Encodable-&gt;&gt;Container: call encode passing field name and value\n        Container-&gt;&gt;JSONFuture: append value\n    end\n\n    JSONEncoderImpl-&gt;&gt;JSONFuture: get value\n    JSONFuture-&gt;&gt;JSONEncoderImpl: return JSONValue\n    JSONEncoderImpl-&gt;&gt;JSONEncoder: JSONValue\n    JSONEncoder-&gt;&gt;User: Data\n\n\n\n\n\nFor each nested user type encountered during serialization a new subprocess starts with wrapEncodable method.\nUtilizing a customized encode method within the ContactInfo struct provides us with greater insights into the encoding mechanics. By default, when we conform a struct to the Encodable protocol, Swift automatically synthesizes standard encode methods through internal reflection. This custom approach allows us to observe and control the process more closely.\nNow that we know all the necessary details, implementing MFEncoder will be quite straightforward. In the last part we will discuss some minor details of our implementation which are specific for multipart/form-data as well as show our approach to testing those with Playgrounds."
  },
  {
    "objectID": "posts/mfencoder/part_1.html",
    "href": "posts/mfencoder/part_1.html",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "",
    "text": "In this set of articles, we take a close look at Swift programming. We’re focusing on creating a package called “MFEncoder” to handle multipart form data. This is important for things like sending forms and files over the internet. We will use the OpenAI GPT-4 chatbot as a guide or mentor to help us through the project.\n\n\n\nSwift is a popular language for making software, especially for Apple’s devices like iPhones and Macs. Multipart forms help us to send more complex data from a client to a server. Right now, there’s a gap in available tools that make working with multipart forms in Swift easy and efficient. Our package, “MFEncoder,” aims to fill that gap.\nThe final result can be found here\n\n\n\n\nTo understand the real-world challenges and basic ideas behind using Swift for data encoding.\nTo build “MFEncoder,” a tool for making multipart form data handling easier in Swift.\nTo explore how OpenAI’s GPT-4 can act as a virtual mentor to help us complete this project.\n\n\n\n\nWe’ll cover these topics in the series:\n\nStarting Points: We’ll talk about the basics of data encoding and multipart forms.\nDeep Dive into MFEncoder: We’ll explain how the package works and what features it offers.\nLearning from a Virtual Mentor: How OpenAI’s chatbot has guided us in this project.\nTesting and Performance: We’ll discuss how we know the package works well.\nWrap-Up and Next Steps: We’ll summarize what we’ve done and talk about what could come next.\n\n\n\n\nThis set of articles is both a learning resource and a practical guide. It’s designed to help anyone interested in Swift programming by providing both basic knowledge and practical examples. Plus, the package we’re making will meet a real need for those looking to work with multipart forms in Swift.\nArtem Putilov, 2023"
  },
  {
    "objectID": "posts/mfencoder/part_1.html#brief-summary",
    "href": "posts/mfencoder/part_1.html#brief-summary",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "",
    "text": "In this set of articles, we take a close look at Swift programming. We’re focusing on creating a package called “MFEncoder” to handle multipart form data. This is important for things like sending forms and files over the internet. We will use the OpenAI GPT-4 chatbot as a guide or mentor to help us through the project."
  },
  {
    "objectID": "posts/mfencoder/part_1.html#what-were-doing",
    "href": "posts/mfencoder/part_1.html#what-were-doing",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "",
    "text": "Swift is a popular language for making software, especially for Apple’s devices like iPhones and Macs. Multipart forms help us to send more complex data from a client to a server. Right now, there’s a gap in available tools that make working with multipart forms in Swift easy and efficient. Our package, “MFEncoder,” aims to fill that gap.\nThe final result can be found here"
  },
  {
    "objectID": "posts/mfencoder/part_1.html#goals",
    "href": "posts/mfencoder/part_1.html#goals",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "",
    "text": "To understand the real-world challenges and basic ideas behind using Swift for data encoding.\nTo build “MFEncoder,” a tool for making multipart form data handling easier in Swift.\nTo explore how OpenAI’s GPT-4 can act as a virtual mentor to help us complete this project."
  },
  {
    "objectID": "posts/mfencoder/part_1.html#how-well-do-it",
    "href": "posts/mfencoder/part_1.html#how-well-do-it",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "",
    "text": "We’ll cover these topics in the series:\n\nStarting Points: We’ll talk about the basics of data encoding and multipart forms.\nDeep Dive into MFEncoder: We’ll explain how the package works and what features it offers.\nLearning from a Virtual Mentor: How OpenAI’s chatbot has guided us in this project.\nTesting and Performance: We’ll discuss how we know the package works well.\nWrap-Up and Next Steps: We’ll summarize what we’ve done and talk about what could come next."
  },
  {
    "objectID": "posts/mfencoder/part_1.html#why-this-matters",
    "href": "posts/mfencoder/part_1.html#why-this-matters",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "",
    "text": "This set of articles is both a learning resource and a practical guide. It’s designed to help anyone interested in Swift programming by providing both basic knowledge and practical examples. Plus, the package we’re making will meet a real need for those looking to work with multipart forms in Swift."
  },
  {
    "objectID": "posts/mfencoder/part_1.html#what-is-a-multipart-form",
    "href": "posts/mfencoder/part_1.html#what-is-a-multipart-form",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "What is a multipart form?",
    "text": "What is a multipart form?\nMultipart form data is typically used when sending forms that have more complex data, such as files. It’s called “multipart” because the different parts of the form are sent as different parts in the body of the HTTP request, with each part having its own content type and other headers.\nIn order to construct a multipart/form-data payload, we need to:\n\nSet the HTTP header Content-Type to multipart/form-data; boundary=someArbitraryString, where someArbitraryString is a string that doesn’t occur in the data.\nEncode the data for each part with the following format:\n--someArbitraryString\nContent-Disposition: form-data; name=\"fieldName\"; filename=\"filename.jpg\"\nContent-Type: image/jpeg\n\n...data...\nThe --someArbitraryString line denotes the start of a part. The Content-Disposition header specifies the name of the form field and, if applicable, the filename. The Content-Type header specifies the MIME type of the data. After the headers, an empty line is required, and then the data for the part can follow. We need to make sure the boundary doesn’t appear in our data, and to use for line breaks, as required by the HTTP specification.\nAfter all parts, add a final boundary with an extra -- at the end: --someArbitraryString--\n\n\nMIME and Content-Disposition: A Brief History\nMIME (Multipurpose Internet Mail Extensions) and Content-Disposition are quite ancient in the context of internet technologies, dating back to the early days of the World Wide Web and even earlier to email systems.\nMIME was initially created to extend the format of email messages to support text in character sets other than ASCII, as well as attachments of audio, video, images, and application programs. Before MIME, email was largely constrained to the English language and could not easily handle multimedia attachments. MIME solved this by specifying how different types of content could be encoded for transport.\n\n\nContent-Disposition: A Child of MIME\nThe Content-Disposition header originated as a way to specify the disposition of MIME-encoded message parts. It was used to specify whether a certain piece of content should be displayed inline within the email client or treated as an attachment. It also allowed for the naming of attached files.\n\n\nHow Did We Get Here?\nOver time, the HTTP protocol borrowed many ideas from MIME due to the similarity in their requirements. HTTP often deals with the transportation of various types of content just like email, and therefore many MIME headers, including Content-Type and Content-Disposition, became useful in HTTP as well.\n\nWhy Similar?\n\nExtensible: Both HTTP and email needed a way to handle an arbitrary number of types of content, so the extensible nature of MIME was a good fit.\nSelf-Describing: MIME makes each message part self-describing. This is crucial in both email and HTTP where a message may go through several intermediaries, and each needs to be able to understand the content to some extent.\nMulti-Part Messages: Both email and HTTP forms had to solve the problem of “multipart” messages (i.e., messages containing multiple pieces of data in potentially different formats). MIME offered a standardized solution for this with multipart/* types.\nCharacter Sets: With the globalization of the internet, the need for handling multiple character sets became evident. MIME had already solved this problem for email, so it made sense to reuse the same solution for HTTP.\nBinary Data: Both email and HTTP had the need to transmit binary data. MIME provides a mechanism to encode this kind of data for transport.\n\nSo, while MIME and Content-Disposition may seem dated, they provide a tried-and-true standard for handling a wide variety of content types across different internet technologies. And because they were designed to be extensible, they have been able to evolve and remain relevant even as the internet has grown and changed."
  },
  {
    "objectID": "posts/mfencoder/part_1.html#how-to-produce-it-with-swift",
    "href": "posts/mfencoder/part_1.html#how-to-produce-it-with-swift",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "How to produce it with Swift",
    "text": "How to produce it with Swift\nGPT-4 suggests the following example which we will be using as a starting point:\nvar request = URLRequest(url: URL(string: \"https://example.com\")!)\nrequest.httpMethod = \"POST\"\n\nlet boundary = \"Boundary-\\(UUID().uuidString)\"\nrequest.setValue(\"multipart/form-data; boundary=\\(boundary)\", forHTTPHeaderField: \"Content-Type\")\n\nvar body = Data()\n\nlet fieldName = \"myField\"\nlet fieldValue = \"myValue\"\nbody.append(\"--\\(boundary)\\r\\n\".data(using: .utf8)!)\nbody.append(\"Content-Disposition: form-data; name=\\\"\\(fieldName)\\\"\\r\\n\\r\\n\".data(using: .utf8)!)\nbody.append(\"\\(fieldValue)\\r\\n\".data(using: .utf8)!)\n\nlet fileUrl = URL(fileURLWithPath: \"/path/to/your/file.jpg\")\nlet fileData = try! Data(contentsOf: fileUrl)\nlet fileName = fileUrl.lastPathComponent\nbody.append(\"--\\(boundary)\\r\\n\".data(using: .utf8)!)\nbody.append(\"Content-Disposition: form-data; name=\\\"file\\\"; filename=\\\"\\(fileName)\\\"\\r\\n\".data(using: .utf8)!)\nbody.append(\"Content-Type: image/jpeg\\r\\n\\r\\n\".data(using: .utf8)!)\nbody.append(fileData)\nbody.append(\"\\r\\n\".data(using: .utf8)!)\n\nbody.append(\"--\\(boundary)--\\r\\n\".data(using: .utf8)!)\n\nrequest.httpBody = body\n\nlet task = URLSession.shared.dataTask(with: request)\ntask.resume()\nThis example teaches us several important points:\n\nTo convert our form into request body we us Data object in Swift\nWe convert each form field name and value to smaller Data chunks and append them to our body\nWe can encode any string values using utf8\nWe can use file URLs as primary media for passing any files like images, documents etc\nWe use same Data object to read the contents of file, passing its URL as parameter\nWith each field name we add boundary and Content-Disposition row\nFor files its also important to add filename and MIME type\n\nLast point poses the next problem."
  },
  {
    "objectID": "posts/mfencoder/part_1.html#how-to-detect-mime-type-of-an-arbitrary-file-in-swift",
    "href": "posts/mfencoder/part_1.html#how-to-detect-mime-type-of-an-arbitrary-file-in-swift",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "How to detect MIME type of an arbitrary file in Swift?",
    "text": "How to detect MIME type of an arbitrary file in Swift?\nSolution that GPT-4 suggested here was marked as using some deprecated tech by Apple\nimport MobileCoreServices\n\nfunc mimeType(forPath path: String) -&gt; String {\n    let url = URL(fileURLWithPath: path)\n    let pathExtension = url.pathExtension\n\n    if let uti = UTTypeCreatePreferredIdentifierForTag(kUTTagClassFilenameExtension, pathExtension! as CFString, nil)?.takeRetainedValue() {\n        if let mimetype = UTTypeCopyPreferredTagWithClass(uti, kUTTagClassMIMEType)?.takeRetainedValue() {\n            return mimetype as String\n        }\n    }\n    return \"application/octet-stream\"\n}\nAs an alternative GPT-4 suggested using hardcoded mappings or wrapping low level C libraries like libmagic. Luckily we were able to find a replacement for deprecated CarbonCore tools - UTType, which can be obtained via URL.resourceValues method as follows:\nfunc getMimeTypeFromURL(_ fileURL: URL) -&gt; String? {\n  var fileURL = fileURL\n  fileURL.resolveSymlinksInPath()\n  do {\n    let fileObjectResource = try fileURL.resourceValues(forKeys: [.isDirectoryKey])\n    if fileObjectResource.isDirectory ?? false {\n      return nil\n    }\n    if #available(macOS 11.0, iOS 14.0, *) {\n      let resourceValues = try fileURL.resourceValues(forKeys: [.contentTypeKey, .isDirectoryKey])\n      if let type = resourceValues.contentType, let mime = type.preferredMIMEType  {\n        return mime\n      }\n    }\n    return \"application/octet-stream\"\n  } catch {\n    return nil\n  }\n}\nUTType api is only supported since iOS 14.0 so we fallback to generic application/octet-stream for any previous versions. Providing accurate MIME type is not necessary for properly uploading files, but it can be crucial in some cases (like if backend applies some compression over the uploaded media). We added resolveSymlinksInPath method after we encountered a bug with bundled resource. At least in some cases bundled resources were symlinks and we were getting application/octet-stream type for them unless those symlinks were resolved."
  },
  {
    "objectID": "posts/mfencoder/part_1.html#using-interactive-playground-to-test-drive-our-mime-detection-and-form-serialization",
    "href": "posts/mfencoder/part_1.html#using-interactive-playground-to-test-drive-our-mime-detection-and-form-serialization",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "Using interactive Playground to test-drive our MIME detection and form serialization",
    "text": "Using interactive Playground to test-drive our MIME detection and form serialization\nWe wanted to make sure our getMimeTypeFromURL function works with real life files. We also wanted to play around with an idea of embedding interactive UI into Swift Playground. We decided to create a basic View in SwiftUI that would allow us to do 2 things:\n\nPick a random directory from our hard drive and run its contents by getMimeTypeFromURL to see how it handles different files\nProvide a UI for filling basic form that we will be encoding and sending to the sample API in order to verify the serialization works. We picked FastAPI and its underlying Starlette http server as a source of truth to see if our multipart/form-data can be recognized and parsed.\n\nAdding SwiftUI view to a Playground is as simple as\n// 1. Import the SwiftUI\nimport SwiftUI\n\n// 2. Import the Playground Support\nimport PlaygroundSupport\n\n// 3. Build the View\n\nstruct ContentView: View {\n  \n  @State private var formValues: FormValues = FormValues()\n  \n  var body: some View {\n    // TODO: add our controls here\n  }\n}\n\n// 4. Attach it to the Playground\nPlaygroundPage.current.setLiveView(ContentView())\nAfter that we run our Playground and voila\n\n\n\nSwiftUI In Playground\n\n\nOpening the standard file selection dialog inside our View was actually a little bit more challenging. GPT-4 was suggesting some convoluted ways with NSViewRepresentable as proxy between AppKit NSOpenPanel and our SwiftUI view. After some trials and errors we came up with much simpler way:\n\n// we simply initialize NSOpenPanel and open in as modal, than call our callback when its over\nfunc openFolderOrFile(canChooseDirectories: Bool = false, _ action: (_ url: URL) -&gt; Void) {\n  let openPanel = NSOpenPanel()\n  openPanel.title = \"Choose a directory\"\n  openPanel.showsResizeIndicator = true\n  openPanel.showsHiddenFiles = false\n  openPanel.canChooseDirectories = canChooseDirectories\n  openPanel.canCreateDirectories = false\n  openPanel.allowsMultipleSelection = false\n  openPanel.directoryURL = URL(fileURLWithPath: NSHomeDirectory())\n  \n  if openPanel.runModal() == NSApplication.ModalResponse.OK {\n    if let url = openPanel.url {\n      action(url)\n    }\n  }\n  \n}\n\n\n\n// ... deep in our View we use it like this...\n    Button(\"Select Folder\") {\n        openFolderOrFile(canChooseDirectories: true) {\n          url in\n          print(\"Selected directory: \\(url.path)\")\n          walkFolder(url)\n          \n        }\n      }\nWe use FileManager to read the contents of a directory and iterate files inside. This works quite well, except in some cases the Playground gets frozen. Seems like it mostly happens when XCode is in full screen mode.\nHaving all the necessary tools we were ready to start implementing MFFormData - A low-level API, inspired by the Web’s FormData API, that gives complete control over the form data before submitting it over HTTP.\nWe will tell about it in our next part of this series."
  },
  {
    "objectID": "posts/holodeck-filterbuilder/index.html",
    "href": "posts/holodeck-filterbuilder/index.html",
    "title": "Holodeck Inc. Filter Builder",
    "section": "",
    "text": "Holodeck Inc. is changing the game in the entertainment industry with its revolutionary holodeck technology. With the help of advanced virtual reality and haptic feedback, people can now experience their wildest dreams and fantasies in a completely immersive environment.\nAs a software engineer at Holodeck Inc., I’m part of the team that builds the platform that powers this cutting-edge technology. One of our main challenges is to provide a seamless and intuitive experience for users to create and customize their own virtual environments.\nThat’s where our FilterBuilder comes in. It’s a powerful tool that enables users to easily create complex search queries to filter and sort their virtual environments. With a simple drag-and-drop interface, users can add various criteria and conditions to narrow down their search, from the type of environment to the number of people allowed and the specific activities available.\nBehind the scenes, our FilterBuilder is built on top of ElasticSearch, a powerful search engine that allows us to efficiently search through vast amounts of data in real-time. We’ve developed a custom adapter that translates the FilterBuilder structures into ElasticSearch DSL queries, making it easy for us to process and respond to search requests quickly and accurately.\nThe FilterBuilder itself is built on a recursive and polymorphic architecture, allowing it to handle a wide variety of search criteria and conditions. We use abstract classes and factory functions to dynamically generate the proper concrete classes based on the specific type of filter attribute and schema reference.\nFor example, the ValuesListHandler class handles filters that allow users to select from a pre-defined list of options. It generates ElasticSearch DSL queries that search for records that match any of the selected options.\nexport class ValuesListHandler&lt;T&gt; extends AbstractFieldHandler&lt;ValuesListItemType&lt;T&gt;[], ValuesListItemType&lt;T&gt;[]&gt; {\n    public asSearchRequest(): Object {\n        return {\n            terms: {\n                [`${this.name}.keyword`]: this.value?.map((item) =&gt; item.value),\n            },\n        };\n    }\n\n    public buildFilterInputInitializerAggregation() {\n        return this.buildAggregationQuery({\n            terms: {\n                field: `${this.name}.keyword`,\n                size: 1000,\n            },\n        });\n    }\n\n    public processElasticInputInitializerResponse(response: ValuesListInitializerResponse) {\n        if (response.aggregations) {\n            const a = response.aggregations[this.name];\n            if (Array.isArray(a.buckets)) {\n                return a.buckets.map((item) =&gt; ({\n                    label: `${item.key} (${item.doc_count})`,\n                    value: item.key,\n                }));\n            }\n        }\n    }\n}\nThe FilterBuilder also includes a progress indicator endpoint that shows users how many records match each sub-block in the filter. We achieve this by running multiple smaller queries that progressively narrow down the search criteria, and aggregating the results to provide real-time feedback on how many records are matched by each block.\nOur FilterBuilder is just one small part of the Holodeck platform, but it plays a crucial role in providing a seamless and immersive experience for our users. With its powerful and intuitive interface, users can quickly and easily create their own virtual environments and experiences, making the impossible possible.\n\n\n\n  Artem Putilov, 2023"
  },
  {
    "objectID": "posts/holodeck-elastic/index.html",
    "href": "posts/holodeck-elastic/index.html",
    "title": "Holodeck Inc. Elastic Queries",
    "section": "",
    "text": "— Can you tell me a little bit about your current project? What are its goals, what technologies are you using, and what challenges are you facing?\nSure! I’m currently working as a software architect and fullstack engineer at Holodeck Inc. We are revamping Holodeck Inc ERP system. It is a highly customised platform that was initially built on PHP based CMS, but now we are doing it from scratch using micro-services architecture.\n— Great! It sounds like an interesting project. Microservices architecture can bring many benefits such as scalability, maintainability and ease of deployment, but it also comes with challenges such as increased complexity, difficulty in monitoring and debugging, and dealing with communication and data consistency between services. How are you approaching these challenges in your project?\nWe decided to use event sourcing as a core of entire system. Event-sourcing suites very well for micro-services platforms. To complement this we also chose a CQRS approach to building our new api.\n— That’s a smart choice! Event sourcing and CQRS can help to manage the complexity of a microservices system and ensure data consistency between services. Event sourcing helps to maintain a complete and verifiable history of the changes to the data in the system, while CQRS allows you to separate the handling of read and write operations, providing better scalability and performance. What technology stack have you decided to use for implementing this new system?_—\nGreat question! We chose MongoDB as our main source of truth database for storing all the events there. As a projections database we decided to go with ElasticSearch. We picked it for its great performance qualities but also because it provides many useful search tools out of the box, and search plays a great part in Holodeck ERP Platform.\n— MongoDB and Elasticsearch are both popular choices for event sourcing and CQRS, so it sounds like you have a solid technology stack. MongoDB provides strong support for document-based storage and is a good choice for storing the events, while Elasticsearch is known for its fast search capabilities and can handle the projection of the data into the right format for your read models. What programming language(s) and frameworks are you using to build the microservices and API?—\nWe were picking between Kafka and RabbitMQ for a broker, but decided to go with RabbitMQ finally. As main language we have TypeScript for both backend and frontend. Specifically we use NestJS on backend and React and React.Native for the Dashboard and a couple of mobile apps.\n— RabbitMQ and TypeScript are both widely used and reliable technologies. NestJS is a great choice for building scalable and maintainable backend applications, and React is a popular frontend library for building user interfaces. Using TypeScript for both backend and frontend is a good idea as it helps to ensure consistency and reduces the risk of type-related errors. Please tell me more.\nWell we actually developed quite interesting solution for read operations: we wanted to avoid unnecessary duplication of logic on frontend and backend and decided to go with a kind of a hybrid approach: while in the write mode most of the business rules is implemented (and enforced) on backend, for read side we mostly implement business logic on a frontend side. Does it make sense to you?\n— Yes, that can make sense in some situations. By implementing the read-side business logic on the frontend, you can reduce the load on the backend, increase the responsiveness of the frontend, and improve the performance of the overall system. — However, it’s important to keep in mind that implementing business logic on the frontend can also increase the complexity of the frontend code and make it more difficult to maintain and test. Additionally, it can make it harder to enforce security and access control, as the backend is not involved in the processing of read operations. — It’s a trade-off between the benefits and drawbacks, and the best approach will depend on the specific requirements and constraints of your project.\nYes exactly! But because projections were already sanitised in some way (they only contain data necessary for frontend to read) we were able to develop very universal security rules on a backend api that would allow frontend to directly fetch the necessary information from projections. Frontend would compile queries on ElasticSearch DSL and then send them to specified endpoint. The only thing backend do is obtain a user’s role from the token and check if such role is allowed to access the specified index. This way we implemented additional restrictions on a “table” (index in ElasticSearch terms) level. We could also implement record level restrictions by injecting short filters in the queries but so far it was not necessary.\n— That sounds like a good approach for balancing the responsibilities of the frontend and backend. By allowing the frontend to fetch the data directly from the projections database, you can reduce the load on the backend and improve performance, while still enforcing security and access control through the backend API. — By using ElasticSearch’s search DSL, you can ensure that the frontend is able to easily and efficiently access the data it needs, while still being able to apply any necessary restrictions and filters. — It’s a good example of how careful consideration of the trade-offs can result in a well-designed solution that strikes a balance between different requirements and constraints.\nThank you! In some way this approach was inspired by GraphQL and Firebase Realtime DB solutions\n— That’s great to hear! Both GraphQL and Firebase Realtime Database have become popular solutions for building scalable and performant web and mobile applications. By taking inspiration from these technologies, you can leverage some of their strengths and benefits to build a solution that meets the specific needs of your project. — Overall, it sounds like you have a solid plan and well-thought-out approach to building the new ERP system. Tell me more about it!\nStill there was one issue that we had to address: that is a separation of concerns. While we decided to go with ELK + MongoDB and even agreed to use ElasticSearch DSL as our only way to send queries to backend, we didn’t want our business logic to be polluted with ElasticSearch specifics. We wanted to follow the clean architecture principles which meant that our business rules should be as pure as possible.\n— That’s a great point. Keeping the business logic separate from the technical details of the data storage and retrieval can make it easier to maintain, test, and change the code in the future. By following the clean architecture principles, you can ensure that the code is modular and well-organized, which can improve its overall quality and reliability.\n— To address this issue, you can use an intermediary layer between the business logic and the data storage. This layer can be responsible for transforming the business logic into ElasticSearch queries and then executing them, and for transforming the ElasticSearch results into the format that the business logic expects. By doing this, you can keep the business logic abstracted from the specifics of the data storage and retrieval, and make it easier to change either of these components in the future if needed.\nThats exactly what we did! We implemented an abstract Query interface that would be responsible for anything that has to do with querying data, including sorting, filtering and grouping. This interface was in our second (ports) layer of architecture where all the abstract interfaces reside mostly used for dependency injections. Than we had our use cases that would use plain business entities and abstracts of services to actually run the business logic and make all the communication possible. Use cases would build abstract queries and then give them to abstract services to obtain the necessary data. In the real app the factories would initialise the real implementations of those abstract services and “inject” them inside those use cases. During test and development however we would use mocked services instead.\n— That’s great to hear! By using the abstract query interface, you are making the business logic independent of the specifics of the data storage and retrieval. This can help to improve the maintainability, testability, and flexibility of the code.\n— By using dependency injection and mocking the services during tests, you can further decouple the components and make it easier to test the business logic in isolation. This can help to improve the overall quality and reliability of the code, and make it easier to change or extend the system in the future.\n— Overall, it sounds like you have a well-designed and thought-out architecture for the new ERP system. I hope that it serves you well and helps you to build a highly scalable and performant solution.\nTo process the abstract queries we actually implemented 2 kinds of adapters. First would use universal ElasticSearch Service and prepare ElasticSearch DSL for it to send the queries to backend. The other one was able to process the same queries and search for Javascript Objects inside Arrays. So it was In-Memory query adapter which we could use interchangeably with Elastic Query Adapter when it was more reasonable to first fetch all existing records and then let users search / sort / group them on frontend (in case collections where not expected to be huge or in case the data was coming from the sources other then ElasticSearch).\n— That’s a good approach! By implementing two different adapters for processing the abstract queries, you can easily switch between the two depending on the use case.\n— For example, if you expect the collections to be huge, it would be more efficient to use the ElasticSearch adapter and offload the searching and sorting to the backend. On the other hand, if you have smaller collections, or if the data is coming from sources other than ElasticSearch, you can use the In-Memory adapter and perform the searching and sorting on the frontend.\n— This level of flexibility can be very useful, especially when dealing with different use cases and requirements, as you can easily change the implementation of the queries processing without affecting the rest of the system. It can also help to improve the performance and scalability of the system by choosing the most appropriate adapter for each use case.\nExactly! We were also able to achieve quite good results with using Elastic Buckets aggregations for presenting the data as grouped collection\n— That’s great! Using Elastic Buckets aggregations is a very powerful feature of ElasticSearch, as it allows you to perform complex data aggregations, such as grouping and summarizing, in a highly efficient manner. By using this feature, you can present the data as grouped collections, making it easier for the users to analyze and understand the data.\n— Furthermore, aggregations are performed directly in ElasticSearch, which can significantly reduce the amount of data that needs to be transferred over the network, improving the overall performance of the system. By combining event-sourcing, CQRS, and aggregations, you can build a highly efficient and flexible system that can effectively handle large amounts of data.\nSo here is some fragment of our query interfaces responsible for grouping. What can you make of it?\nexport interface GroupingOption {\n    schemaRef: GroupableSchemaItem;\n\n    defaultApplied?: boolean;\n}\n\nexport interface GroupingAggregateOption {\n    schemaRef: GroupableSchemaItem;\n    aggregate: AggregatorType;\n}\n\nexport interface GroupingSetup {\n    groupings: GroupingOption[];\n    aggregates?: GroupingAggregateOption[];\n}\n\nexport interface GroupableSchemaItem extends FilterSchemaItem {\n    groupByType: GroupByOptionType;\n}\n\nexport enum GroupByOptionType {\n    Numeric = 'numeric',\n    Value = 'value',\n    Date = 'date',\n    Age = 'age',\n}\n\nexport interface FilterSchemaItem {\n    fieldName: string;\n    fieldLabel: string;\n\n}\n— This code defines interfaces and enums related to grouping and aggregating data. The GroupingOption interface holds information about the schema item to be grouped and whether it’s the default grouping option. The GroupingAggregateOption interface holds information about the schema item to be grouped and the type of aggregate operation to be performed on it. The GroupingSetup interface holds the information about the groupings and aggregates to be performed. The GroupableSchemaItem interface extends the FilterSchemaItem interface and adds a groupByType property to specify the type of grouping. The GroupByOptionType enum defines the different types of grouping that can be performed (numeric, value, date, age). The FilterSchemaItem interface holds information about the field to be filtered, including its name and label.\nThats right! So as you know the multilevel grouping can quickly become very problematic in terms of a pagination of a data. If your collection is big enough (and Holodeck operates with millions of records of HoloCharacters items in their collection) and you decide to group it by more than one level than you have a problem - how best to present this data in a tree like table.\n— Yes, it can be challenging to paginate and present grouped data in a tree-like table, especially when dealing with large collections. One potential solution could be to implement lazy loading for the lower levels of the tree. This way, the initial load can be optimized, and the additional levels can be loaded on demand, improving the overall performance and user experience. Another solution could be to implement a combination of client and server-side pagination, where only a portion of the data is loaded and processed on the client side, while the rest is loaded from the server as needed.\nWe did a bit of a first approach although not quite that. We didn’t want to load final entities only at the moment when user finally gets to their level. After all we are expected to present the entities, not the closed groups. So we decided to have a hybrid approach: we do fetch some amount of final level entities (tree leaves) immediately on first load but we also provide a “See All” feature for each group so that if this particular group of entities of of interest, user would click on See All link and the table would switch from grouped tree to a flat one with all the nested groupings turned into a filters.\n— That sounds like an efficient way to approach the problem. By fetching only some of the final level entities, you can reduce the amount of data that needs to be loaded initially, while still giving the user the option to see all of the data if they need to. And by allowing the user to switch from a grouped tree-like view to a flat view with filters, you can provide more flexible and intuitive ways for them to access the data.\nWe used ElasticSearch top_hits aggregation to fetch the first most suitable items in each group.\n— Yes, top_hits aggregation is a good choice for fetching a limited number of the most relevant documents in each group. It allows you to retrieve a specified number of the top scoring documents for each group, which can be useful in cases where you need to display a representative sample of documents from each group, such as in your use case.\nHere is some more code for you. This is a method of our Elastic Query Adapter that is responsible for building grouping DSL:\nprivate buildGrouping(grouping: GroupingSetup): SearchRequest {\n    const columnAggregations = this.buildAggregations(grouping.aggregates ?? []);\n    const { groupings } = grouping;\n    const results = {\n        top_hits: {\n            size: this.settings.groupingSetup.topHitsCount,\n        },\n    };\n    let aggs: Record&lt;string, AggregationsAggregationContainer&gt; = {};\n\n    for (let index = groupings.length - 1; index &gt;= 0; index--) {\n        const item = groupings[index];\n        if (index === groupings.length - 1) {\n            aggs.results = results;\n        }\n        if (columnAggregations) {\n            Object.assign(aggs, columnAggregations);\n        }\n\n        const key = this.makeAggKeyFromFieldName(item.schemaRef.fieldName);\n\n        const GroupingHandlerClass = groupByHandlersFactory(item.schemaRef);\n        const block = new GroupingHandlerClass(this.settings.groupingSetup).buildGroupingBlock(item);\n\n        aggs = { [key]: Object.assign({}, block, { aggs: { ...aggs } }) };\n    }\n    return { aggs };\n}\nThis code builds the Elasticsearch aggregation for grouping based on the grouping setup passed to the method buildGrouping. The method starts with building the column aggregations using the buildAggregations method. Then, it creates a top_hits aggregation that will retrieve the first topHitsCount number of items in each group.\nNext, the code loops through the groupings in reverse order and adds the grouping aggregation to the aggs object. The key of the aggregation is created using the makeAggKeyFromFieldName method. The grouping block is then built using a GroupingHandlerClass, which is obtained from the groupByHandlersFactory method. The buildGroupingBlock method of the GroupingHandlerClass is then called to build the grouping block, which is added to the aggs object. The final aggs object is returned as part of the SearchRequest.\nAnd this is the method that unwraps the grouped results coming from ElasticSearch:\n    private extractSubrows&lt;T&gt;(\n        aggregation: Record&lt;string, ElasticAggregateBucketsContainer&lt;T&gt;&gt;,\n        grouping: GroupingSetup,\n        level = 0\n    ) {\n        const g = grouping.groupings[level];\n        const isLastLevel = level &gt;= grouping.groupings.length - 1;\n        if (g) {\n            const aggKey = this.makeAggKeyFromFieldName(g.schemaRef.fieldName);\n            const bucketsContainer = aggregation[aggKey];\n            const BucketsExtractor = groupByHandlersFactory(g.schemaRef);\n\n            const buckets = new BucketsExtractor(this.settings.groupingSetup).extractBuckets(bucketsContainer);\n\n            const subRows = buckets.map((b) =&gt; {\n                const result: ElasticGroupingResponse&lt;T&gt; = {\n                    cells: this.extractAggregatedValues(b, grouping.aggregates ?? []),\n                    value: b.key,\n                    subrows_count: b.doc_count,\n                    fieldName: g.schemaRef.fieldName,\n                    isGrouped: true,\n                    subrows: isLastLevel\n                        ? ElasticSearchService.extractSources(b.results!).map((r) =&gt; ({\n                              cells: r,\n                              isGrouped: false,\n                              fieldName: '',\n                              subrows_count: 0,\n                              value: '',\n                              subrows: [],\n                          }))\n                        : this.extractSubrows(b, grouping, level + 1),\n                };\n                return result;\n            });\n            return subRows;\n        } else {\n            return [];\n        }\n    }\nThis code looks is implementing the recursive extraction of subrows in a grouped response from ElasticSearch. The extractSubrows method takes an aggregation object, which is a representation of the ElasticSearch aggregation response, and a grouping object, which holds the grouping setup information, and the current level in the grouping hierarchy.\nFor each level, the method finds the relevant grouping field based on the level parameter and the grouping object, then uses the grouping field to determine the correct handler (BucketsExtractor) to extract the buckets from the aggregation response. The extracted buckets are then processed to form ElasticGroupingResponse objects, which contain the aggregated values and subrows for each group.\nIf the current level is the last level in the hierarchy, the method extracts the sources from the results property of each bucket and maps them to new ElasticGroupingResponse objects with the isGrouped property set to false, indicating that they are not grouped. If the current level is not the last level, the method calls extractSubrows recursively with the next level.\nThere was also some interesting pagination solution we also implemented\n— Sure, I’d love to hear about it. Can you please provide some code or explain the implementation details?\nWell as you know ElasticSearch has a simple pagination mechanism similar to other databases. It uses from and size parameters.\n— Yes, that’s correct. The from parameter specifies the starting point for the result set and the size parameter specifies the number of results to return. Elasticsearch returns the results starting from the from index up to size number of documents.\nHowever according to their documentation using this approach is discouraged for big collections because every time Elastic should go through all the results starting from the first record in collection and then it just drops the unused items up to ‘from’ amount. So the greater from parameter is the slower becomes the query, which means fetching the page 100 of a big collection can be quite long\n— That’s correct, and that’s why an alternative approach is to use a cursor-based pagination mechanism. This involves returning a cursor value that represents the current position in the results, and the client can use this cursor value to paginate forward and backward through the results. This way, the search request only needs to retrieve the results after the cursor, reducing the query time.\nIn ElasticSearch terms this is called search after. And it also involves PIT - point in time to guarantee that each time you request further items in collection, you are actually getting them from the same state of this collection as when you got the first portion of it\n— Yes, that’s correct. The search_after parameter allows you to start the next search from a specific point in the results set, based on the values of one or more sort fields. This is a more efficient way of paginating the results, compared to using from and size.\nCorrect. However we also had to solve the presentation problem here. Using cursor based pagination in terms of UI is best implemented with “Fetch More” paradigm.\n— Yes, that’s right. “Fetch More” allows you to retrieve the next portion of data without reloading the whole page and keeps the UI responsive even when working with big collections. This can be achieved by having a cursor based approach where you keep track of the last fetched item and then request the next portion based on that cursor. This way, ElasticSearch can fetch data from the exact point you left off, making it much faster than the “From and Size” approach.\nBut in our case “Fetch More” was not suitable for us. We are building a ERP class dashboard not the mobile app here. You see what I mean?\n— Yes, I understand. ERP dashboards usually have a more structured layout and often require a different approach to pagination compared to a mobile app. It sounds like you needed a more traditional approach to pagination where users can navigate through multiple pages with specific page numbers.\nYes, so we needed some kind of technique that would look like as if it is using traditional “from/to” pagination but actually would use cursor based one.\n— I see, so you needed a technique that would present the appearance of traditional “from/to” pagination while actually using cursor-based pagination. This allows you to benefit from the efficiency of cursor-based pagination while still providing a familiar interface to the end-user. Did you come up with a solution to this problem?\nYes we did! And we also managed to implement this without breaking our clean architecture approach in a way that all Pagination related techniques were invisible from the business logic and application logic point of view.\n— That’s great! So your solution was to hide the complexity of cursor based pagination behind the traditional from/to pagination interface, while still providing the benefits of cursor based pagination. This way, your application logic and business logic are not impacted and the end-users get a familiar and expected interface for pagination.\nExactly! Our abstract pagination interface that we used in addition to an abstract query interface looks like this:\nexport interface PaginationSetup {\n    pageSize: number;\n    page: number;\n}\nQuite simple, right? It knows nothing about cursors at all. All the implementation depends on a type of Paginator class that we are passing to concrete implementation of our Abstract Service.\n— I understand. So it seems that the PaginationSetup interface only provides information about the page size and the page number to be fetched. The actual implementation of pagination is handled by the concrete Paginator class that is passed to the concrete implementation of the Abstract Service. This allows for a separation of concerns, as the business logic and application logic do not need to worry about the underlying details of pagination, and can instead just work with the simple PaginationSetup interface.\nIn fact we had several implementations of Paginators: one (simple one) was actually using from/size parameters on ElasticSearch. We used it in most cases for relatively small collections. Another was called SearchAfterPaginator and that one was doing cursor based pagination behind the scenes. As you could guess it was implemented by leveraging local memory cache.\n— Yes, that’s a great approach to keep the implementation details of the pagination hidden from the business logic. By having different implementations of the paginator, you can switch between different pagination techniques without affecting the higher-level code. This makes it easy to switch between different techniques depending on the requirements of a specific use case. It also enables you to make improvements or optimizations to the pagination technique without affecting the rest of the code.\nSo our SearchAfter Paginator was using items cache to store the actual items there and then just picking the proper “page” from that local cache. Each time another page was requested it was first checking if it hits the cache or not, and then (if it not) calculate how many more items should be requrested and using the cache to determine the search_after parameter. Then it would fetch new items and store them in cache. And then it would just return the “page” from that cache.\n— I see. That’s a clever solution to the pagination problem. By using a local cache and only fetching new items when needed, you can ensure that each pagination request is fast, while also maintaining the consistency of the data that’s being displayed. This way, you’re able to provide a seamless user experience, even when dealing with large datasets.\nHere is the method responsible for that pagination technique.\npublic paginateRequest(request: ElasticCompiledQueryDTO&lt;BasicQueryDTO&gt;, pagination: PaginationSetup) {\n    const size = pagination.page * pagination.pageSize;\n    const elasticMeta = this.ensureElasticMeta(request);\n    request._elasticRequest.size = Math.max(0, size - elasticMeta.cache.length);\n    if (elasticMeta.cache.length &gt; size) {\n        elasticMeta.cacheHit = true;\n        return request;\n    } else {\n        elasticMeta.cacheHit = false;\n    }\n\n    if (elasticMeta.cache.length &gt; 0) {\n        request._elasticRequest.search_after = this.findSearchAfter(elasticMeta.cache);\n    }\n\n    if (!request._elasticRequest.sort?.length) {\n        request._elasticRequest.sort = [{ _score: 'desc' }] as any;\n    }\n    if (this.includedFields) {\n        request._elasticRequest._source = this.includedFields;\n    }\n    request._elasticRequest.track_total_hits = true;\n    return request;\n}\nThis method is part of the SearchAfterPaginator class, and it’s responsible for adding necessary information to the request object (request) in order to perform cursor-based pagination using the search_after parameter. The pagination object contains information about the desired page size and the desired page number. The method checks if the required items are already stored in the cache (elasticMeta.cache), and if they are, it returns the request object without making a new request to ElasticSearch. If the items are not in the cache, the method calculates the search_after value based on the items already stored in the cache and sets the request size and search_after accordingly, and then returns the updated request object.\nAnd here is where we process the results:\npublic paginateResponse&lt;T&gt;(\n    response: SearchResponse&lt;T&gt;,\n    request: ElasticCompiledQueryDTO&lt;BasicQueryDTO&gt;,\n    pagination: PaginationSetup\n): PaginatedCollection&lt;QueryResultItem&lt;T&gt;&gt; {\n    const elasticMeta = this.ensureElasticMeta(request);\n    if (response) {\n        elasticMeta.cache.push(...response.hits.hits);\n        const total = ElasticSearchService.extractTotalCount(response.hits);\n        if (total) {\n            elasticMeta.cachedTotal = total;\n        }\n    }\n    response.hits.hits = this.sliceHits&lt;T&gt;(elasticMeta.cache, pagination);\n    response.hits.total = elasticMeta.cachedTotal;\n    return ElasticSearchService.elasticHitsToPaginatedCollection(response, pagination);\n}\n— This is a good implementation of Search After pagination. It is efficiently using cache and only fetches what is needed, which saves time and resources. The ensureElasticMeta function makes sure that metadata about the search is stored for later use. The processing of results involves adding to the cache and then slicing the hits to get the desired page of results. The final step is to convert the Elasticsearch hits into a paginated collection. Good job!\nWhat do you think about the quality of the code so far?\n— Overall, the code seems well-organized and well-structured, making use of appropriate data structures and abstraction. It appears to follow the DRY (Don’t Repeat Yourself) principle and make use of helper functions to maintain code cleanliness.\n— It uses type annotations effectively, making it easier to understand the structure of the data being passed between functions.\n— However, there’s room for improvement in a few areas:\n— Some variables and functions could have more descriptive names to improve code readability. — The code would benefit from more comments, explaining the purpose of each section and any complex algorithms used. — The functions could be split into smaller, more focused functions to make the code easier to maintain and test.\n\n\n\n  Artem Putilov, 2023"
  },
  {
    "objectID": "posts/mfencoder/part_2.html",
    "href": "posts/mfencoder/part_2.html",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "",
    "text": "In this set of articles, we take a close look at Swift programming. We’re focusing on creating a package called “MFEncoder” to handle multipart form data. This is important for things like sending forms and files over the internet. We will use the OpenAI GPT-4 chatbot as a guide or mentor to help us through the project.\n\n\n\nIn Part 1 we discussed the details of Multipart form encoding. We also built some foundational elements as well as interactive Playground to help us with testing our implementation with the real backend.\nArtem Putilov, 2023"
  },
  {
    "objectID": "posts/mfencoder/part_2.html#brief-summary",
    "href": "posts/mfencoder/part_2.html#brief-summary",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "",
    "text": "In this set of articles, we take a close look at Swift programming. We’re focusing on creating a package called “MFEncoder” to handle multipart form data. This is important for things like sending forms and files over the internet. We will use the OpenAI GPT-4 chatbot as a guide or mentor to help us through the project."
  },
  {
    "objectID": "posts/mfencoder/part_2.html#previous-parts",
    "href": "posts/mfencoder/part_2.html#previous-parts",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "",
    "text": "In Part 1 we discussed the details of Multipart form encoding. We also built some foundational elements as well as interactive Playground to help us with testing our implementation with the real backend."
  },
  {
    "objectID": "posts/mfencoder/part_2.html#implementation-details",
    "href": "posts/mfencoder/part_2.html#implementation-details",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "Implementation details",
    "text": "Implementation details\nWe implement our MFFormData as a wrapper around an Array of structures. Each item has a key and a value of type Data. We use this array to produce the final Data by joining each item with --boundary separator. Additionally item can hold fileName and mime type.\nWe implement mutating methods (set, append) via set of overloaded methods for all supported data types which are:\n\nCustomStringConvertible: any primitive value including numeric and booleans are covered by this\nURL: adds file contents for file URLs, absolute path string otherwise\nUIImage, NSImage, CGImage\nData: adds file contents with mime type application/octet-stream\nDate: adds date representation\n\nDate serialization is inspired by JSONEncoder via strategy pattern. Supported options are:\n\nUnix timestamp with seconds (default)\nJavascript timestamp (milliseconds)\niso string\ncustom formatter, passed as enclosed value together with option\n\n\nHeaders encoding\nThe headers for each part should be ASCII. This includes the Content-Disposition header, which contains the name of the field and the filename if applicable. The field name and filename should be percent-encoded if they contain non-ASCII or special characters. In Swift, we can use the addingPercentEncoding(withAllowedCharacters:) method for this.\nTo create a custom CharacterSet that includes all ASCII characters in Swift, we can initialize a CharacterSet from a Unicode range. ASCII characters range from 0 to 127, so we can create a CharacterSet that represents all ASCII characters like this:\nlet asciiCharacterSet = CharacterSet(charactersIn: \"\\0\" ... \"\\u{7f}\")\nThis uses the Unicode scalar initializer of CharacterSet and Swift’s support for creating ranges from characters to define a set of all ASCII characters. The range from “\\0” (the null character) to “” (the delete character) includes all ASCII characters.\nPlease note that this CharacterSet will include all ASCII characters, including control characters and other non-printable characters. To exclude these, we should define a more specific range, such as from ” ” (space, ASCII 32) to “~” (tilde, ASCII 126), which includes all printable ASCII characters:\nlet printableAsciiCharacterSet = CharacterSet(charactersIn: \" \" ... \"~\")\n\n\nAccess methods\nTo implement get and getAll methods as well as iterators we need a way to return polymorphic value which basically can be either String or Data.\n  public enum ValueOutput {\n    case stringCase(String)\n    case blobCase(Data)\n    \n    init?(_ item: FormDataItem) {\n      if item.filename != nil {\n        self = .blobCase(item.value)\n      } else {\n        if let stringValue = String(data: item.value, encoding: .utf8) {\n          self = .stringCase(stringValue)\n          \n        } else {\n          return nil\n        }\n      }\n      \n    }\n  }\nThis union enum also serves as an Element of our IteratorsProtocol implementations for values, entities.\nTo implement IteratorProtocol over our Array GPT-4 suggested an elegant way to use deferred increment of a current item counter:\n// part of KeysIterator\n    mutating public func next() -&gt; String? {\n      defer {\n        current += 1\n        while current &lt; elements.count && elements[current - 1].name == elements[current].name {\n          current += 1\n        }\n      }\n      return current &lt; elements.count ? elements[current].name : nil\n    }\nHaving 3 different Iterators with ValueOutput we can complete web-like FormData implementation.\n\n\nHelpers\nTo simplify the process of sending requests we add bodyForHttpRequest and contentTypeForHttpRequest getters. First one actually composes the final Data from wrapped Array, the second one provides contentType header. The reason its useful is because same boundary string should be used inside form data body and content-type header. Additionally we provide complete helper method which is capable of generating URLRequest from given URL as a parameter.\nThis completes our second part. In the next part we will show the implementation of MFEncoder which is a higher level api conforming to Swift Encoder protocol."
  },
  {
    "objectID": "posts/mfencoder/part_4.html",
    "href": "posts/mfencoder/part_4.html",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "",
    "text": "In this set of articles, we take a close look at Swift programming. We’re focusing on creating a package called “MFEncoder” to handle multipart form data. This is important for things like sending forms and files over the internet. We will use the OpenAI GPT-4 chatbot as a guide or mentor to help us through the project.\n\n\n\nIn Part 1 we discussed the details of Multipart form encoding. We also built some foundational elements as well as interactive Playground to help us with testing our implementation with the real backend.\nIn Part 2 we implemented MFFormData - a low-level API, inspired by the Web’s FormData API, that gives complete control over the form data before submitting it over HTTP.\nIn Part 3 we dived into details of Encoder protocol implementation, looking at JSONEncoder source code.\nArtem Putilov, 2023"
  },
  {
    "objectID": "posts/mfencoder/part_4.html#brief-summary",
    "href": "posts/mfencoder/part_4.html#brief-summary",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "",
    "text": "In this set of articles, we take a close look at Swift programming. We’re focusing on creating a package called “MFEncoder” to handle multipart form data. This is important for things like sending forms and files over the internet. We will use the OpenAI GPT-4 chatbot as a guide or mentor to help us through the project."
  },
  {
    "objectID": "posts/mfencoder/part_4.html#previous-parts",
    "href": "posts/mfencoder/part_4.html#previous-parts",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "",
    "text": "In Part 1 we discussed the details of Multipart form encoding. We also built some foundational elements as well as interactive Playground to help us with testing our implementation with the real backend.\nIn Part 2 we implemented MFFormData - a low-level API, inspired by the Web’s FormData API, that gives complete control over the form data before submitting it over HTTP.\nIn Part 3 we dived into details of Encoder protocol implementation, looking at JSONEncoder source code."
  },
  {
    "objectID": "posts/mfencoder/part_4.html#mfvalue-implementation",
    "href": "posts/mfencoder/part_4.html#mfvalue-implementation",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "MFValue implementation",
    "text": "MFValue implementation\nMFValue is a polymorphic recursive enum, which can be thought of as equivalent to JSONValue. The main difference is its Writer class uses MFFormData internally to produce the final result.\n\nNested fields representation\nIn the context of multipart/form-data encoding, there’s no formal, standardized way to handle complex, nested objects directly. The MIME type multipart/form-data is primarily designed for form data that consists of key-value pairs, where the values are either text or file content.\nHowever, there are some commonly used workarounds to include nested or complex data:\n\nFlatten the Keys: The keys for nested objects can be flattened into a string that represents their path within the object. For example, given a nested object like { \"user\": { \"name\": \"Alice\", \"age\": 30 }}, the keys could be flattened as user[name] and user[age].\nMultiple Fields: We can also break the nested structure down into multiple fields with related names, although this can be complex to manage for deeply nested structures. For the same structure we would get user.name and user.age as field names.\nJSON Encoding: Convert the nested objects to a JSON string and send that string as a text field. On the server-side, one can then parse this JSON string back into an object.\n\nHere is an example of possible JSON solution:\nlet nestedData = [\"user\": [\"name\": \"Alice\", \"age\": 30]]\nlet jsonData = try JSONSerialization.data(withJSONObject: nestedData, options: .prettyPrinted)\nlet jsonString = String(data: jsonData, encoding: .utf8)\n\n// Add 'jsonString' to your multipart form\nThe multipart request would look like this:\nContent-Disposition: form-data; name=\"user\"\n{\"name\":\"Alice\",\"age\":30}\n\n\nNested arrays representation\nEncoding a list (array) with multipart/form-data is somewhat similar to encoding nested objects: there is no universally standardized way, but there are some commonly used approaches.\n\nUsing Indexed Keys\nThe most straightforward approach is to use indexed keys for array elements. For example, to encode the array [\"apple\", \"banana\", \"cherry\"] under the key fruits, you might use keys like fruits[0], fruits[1], and fruits[2].\nHere’s how this could look in the HTTP payload:\nContent-Disposition: form-data; name=\"fruits[0]\"\napple\n\nContent-Disposition: form-data; name=\"fruits[1]\"\nbanana\n\nContent-Disposition: form-data; name=\"fruits[2]\"\ncherry\n\n\nUsing Unindexed Keys\nAnother common approach is to use the same key for each item in the array. Some server-side frameworks support this method and will automatically accumulate multiple parameters into a list on the server side.\nHere’s an example:\nContent-Disposition: form-data; name=\"fruits[]\"\napple\n\nContent-Disposition: form-data; name=\"fruits[]\"\nbanana\n\nContent-Disposition: form-data; name=\"fruits[]\"\ncherry\n\n\n\nNestedFieldsEncoding strategy\nTo make our encoder flexible we decided to use provide multiple options of encoding nested fields. We introduced NestedFieldsEncoding strategy enum which currently has 2 options:\n\nflattenFields\nmultipleFields\n\nWe believe that these 2 options cover most of the use cases. While JSON option is something that should be implemented via custom encode methods on user types.\n\nextension MFValue {\n  \n  struct Writer {\n\n    /* ... irrelevant parts omitted ... */\n    func pathToKey(_ path: [String]) -&gt; String {\n      switch nestedFieldsEncodingStrategy {\n      case .flattenKeys:\n        return (path[...0] + path[1...].map({ key in\n          return \"[\\(key)]\"\n        })).joined(separator: \"\")\n        \n      default:\n        return path.joined(separator: \".\")\n      }\n    }\n    func fillFormData(_ value: MFValue, path: [String] = []) {\n      switch value {\n      case .object(let object):\n        for (key, value) in object {\n          \n          var nextPath = path\n          nextPath.append(key)\n          fillFormData(value, path: nextPath)\n        }\n      case .array(let array):\n        precondition(!path.isEmpty, \"Root element should be object\")\n        for (index, value) in array.enumerated() {\n          var nextPath = path\n          if nestedFieldsEncodingStrategy == .flattenKeys {\n            nextPath.append(\"\\(index)\")\n            \n          } else {\n            nextPath[nextPath.endIndex-1] = \"\\(nextPath.last!)[]\"\n          }\n          fillFormData(value, path: nextPath)\n        }\n        \n      case .number(let n):\n        append(path: path, value: n)\n\n    /* ... more basic values cases omitted ...*/\n      }\n    }\n  }\n}\nIf we consider this data:\n{\n    \"users\": [\n        {\"name\": \"Alice\", \"age\": 25},\n        {\"name\": \"Bob\", \"age\": 30}\n    ]\n}\nThen using different NestedFieldEncoding strategy will give us the following results.\n\nflattenFields\nContent-Disposition: form-data; name=\"users[0][name]\"\nAlice\n\nContent-Disposition: form-data; name=\"users[0][age]\"\n25\n\nContent-Disposition: form-data; name=\"users[1][name]\"\nBob\n\nContent-Disposition: form-data; name=\"users[1][age]\"\n30\n\n\nmultipleFields\nContent-Disposition: form-data; name=\"users[].name\"\nAlice\n\nContent-Disposition: form-data; name=\"users[].age\"\n25\n\nContent-Disposition: form-data; name=\"users[].name\"\nBob\n\nContent-Disposition: form-data; name=\"users[].age\"\n30"
  },
  {
    "objectID": "posts/mfencoder/part_4.html#leveraging-playgrounds-for-cross-platform-image-testing",
    "href": "posts/mfencoder/part_4.html#leveraging-playgrounds-for-cross-platform-image-testing",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "Leveraging Playgrounds for Cross-Platform Image Testing",
    "text": "Leveraging Playgrounds for Cross-Platform Image Testing\nWe kicked off this project using a Playground. But when we shifted to Swift Package, we didn’t want to lose the interactive feel. So, we added a new Playground to the package, just as Apple recommended.\nThis Playground does two things: It lets us test the Encoder in real-time and acts as a guide for those using the package.\nAnother bonus of Playgrounds? They’re great for testing across different platforms. We aimed to check if our MFFormData could handle and send all usual image types found in Apple’s world: like NSImage, UIImage, CGImage, and so on. And for the iOS image tests we made a separate iOS Playground.\nBut here’s where it got tricky. When testing various image formats, we hit a snag. If we loaded images directly from an external file system this way:\nlet someImage = UIImage(contentsOfFile: \"/absolute/path/to/someImage.png\")\nAll was good. But when we tried to pull the image from the Playground’s bundle like this:\nlet someImage = UIImage(named: \"someImage.png\") \nNo errors popped up, and the image got sent, but weirdly, the fileType always read application/octet-stream.\nTo fix this, we had to dig deep into XCode’s temporary file system. Turns out, when adding images to the Playground’s bundle, XCode was stashing them in another place and making multiple shortcut links (or “symlinks”) for different building setups. Because of this, our getMimeTypeFromURL was reading a symlink and not the actual image file, leading it to return the wrong mime type.\nWe solved this by calling the resolveSymlinksInPath() function on the files.\nAll in all, using Playgrounds to develop cross-platform libraries turned out to be a smart and efficient move."
  },
  {
    "objectID": "posts/mfencoder/part_4.html#writing-tests-with-gpt-4",
    "href": "posts/mfencoder/part_4.html#writing-tests-with-gpt-4",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "Writing tests with GPT-4",
    "text": "Writing tests with GPT-4\nIt’s a common understanding that writing automated tests is a part of development ideally suited for delegation to AI. We wanted to see this in action, so we asked GPT-4 to draft a test for us. We clarified our focus was solely on the Encoder, meaning we needed to examine the serialization outcome directly. Here’s what GPT-4 came up with:\nimport XCTest\n@testable import YourPackage  // Replace with your actual package name\n\nclass MFFormDataTests: XCTestCase {\n\n    var profile: Profile!\n\n    override func setUp() {\n        super.setUp()\n        profile = Profile()\n    }\n\n    override func tearDown() {\n        profile = nil\n        super.tearDown()\n    }\n\n    func testProfileSerialization() {\n        let formData = MFFormData()\n        formData.append(name: \"username\", value: profile.username)\n        formData.append(name: \"password\", value: profile.password)\n        formData.append(name: \"rank\", value: \"\\(profile.rank)\")\n        formData.append(name: \"active\", value: \"\\(profile.active)\")\n        if let avatar = profile.avatar {\n            formData.append(name: \"avatar\", value: \"\\(avatar)\")\n        }\n\n        // Assuming you have a method to convert this form data to Data or String\n        let serializedData = formData.serialize()  // Replace this with your actual serialization method\n\n        let expectedSerializationResult = \"\"\"\n        --boundary\n        Content-Disposition: form-data; name=\"username\"\n        \\(profile.username)\n        --boundary\n        Content-Disposition: form-data; name=\"password\"\n        \\(profile.password)\n        --boundary\n        Content-Disposition: form-data; name=\"rank\"\n        \\(profile.rank)\n        --boundary\n        Content-Disposition: form-data; name=\"active\"\n        \\(profile.active)\n        --boundary\n        Content-Disposition: form-data; name=\"avatar\"\n        \\(profile.avatar ?? \"\")\n        --boundary--\n        \"\"\"\n\n        XCTAssertEqual(serializedData, expectedSerializationResult)\n    }\n}\nWhile this generated test provides a solid foundation, it’s a bit too basic and needs tweaking. The expectedSerializationResult string is simplified, and we’ll need to refine it to align with the genuine output of our MFFormData class. This adjustment will involve elements like actual MIME types, correct line breaks, and boundary strings. Still, it’s a decent starting point.\nIn the end, we found an improved method to create a fitting expectedSerializationResult for our tests. We utilized our Python web server to cross-check and reflect the payloads we sent from our Playgrounds.\nDistinguishing between the expected and actual strings was tricky, especially when differences revolved around white spacing. Initially, we considered removing all white spaces, but realizing their significance, we sought a better approach. Turning to GPT-4 for guidance, we received a solution that effectively addressed our spacing discrepancies.\nfunc debugPrintString(_ str: String) {\n    var debugString = \"\"\n    for scalar in str.unicodeScalars {\n        if scalar.isASCII {\n            if scalar.value &lt; 32 || scalar.value &gt;= 127 {  // Non-printable ASCII\n                debugString += \"\\\\\\(scalar.value)\"\n            } else {\n                debugString += \"\\(scalar)\"\n            }\n        } else {\n            debugString += \"\\(scalar)\"\n        }\n    }\n    print(debugString)\n}\nOnce we tackled basic serialization, we moved on to testing the more complex MFEncoder. Here, we hit a snag: we couldn’t dictate the sequence in which fields were serialized. This made direct comparison of serialized results challenging for structures with multiple fields. Without a Decoder in our plans, we lacked the means to parse serialization outcomes. Three potential solutions crossed our minds:\n\nTest structures with only one field.\nBypass final result testing and instead check if the MFEncoder was populating the MFFormData correctly. This was a plausible route since we had existing tests for the fundamental encoding.\nDevelop a technique to compare strings that share components but present them in varied sequences, possibly sorting the components in advance.\n\nThe first route was deemed unrealistic. We dismissed the second because it revealed too much about MFEncoder’s inner workings, and we didn’t want our tests to be tied to this particular implementation detail.\nEventually, we realized the third option was the simplest: sorting both strings gave us sequences of characters ready for comparison. This approach gave us the confidence we needed in our results."
  },
  {
    "objectID": "posts/mfencoder/part_4.html#using-ai-as-code-mentor",
    "href": "posts/mfencoder/part_4.html#using-ai-as-code-mentor",
    "title": "Learning Swift Through Building MFEncoder: A Journey Guided by OpenAI",
    "section": "Using AI as code mentor",
    "text": "Using AI as code mentor\nInitially, our strategy was to rely solely on GPT-4 for assistance, a plan that proved effective in approximately 80% of cases. With the guidance of GPT-4, we found little need to consult Stack Overflow. However, we occasionally turned to Apple’s official documentation and conducted a comprehensive examination of the source code for JSONEncoder, as recommended by GPT-4.\nGPT-4 excels at offering high-level explanations of topics such as multipart/form-data and the Encoder protocol. It serves as a valuable supplement to Apple’s formal documentation by providing more accessible language and practical examples. While some of these examples occasionally encountered issues, many of them were resolvable through iterative discussions with GPT-4.\nWe discovered that the psychological effect of having a readily available “expert” via GPT-4 positively influenced our confidence. This made even the more complex tasks appear somewhat less daunting.\nLastly, GPT-4 has been an invaluable asset in crafting both the package documentation and this article. Overall, we are highly optimistic about the potential of learning new technologies and programming languages through this approach."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Fake Innovation",
    "section": "",
    "text": "— I keep having the feeling that every time I work on something, I keep lying to myself and to others. I’ve been working in the IT industry for many years now, and it seems like it’s just spiraling into a self-destructive loop of hype and fake innovation hysteria.\n— The IT industry can certainly be a fast-paced and ever-changing field, with a lot of pressure to constantly innovate and deliver new products and solutions. It’s not uncommon for individuals in this industry to experience feelings of burnout or frustration with the focus on hype and “the next big thing” instead of delivering meaningful and sustainable solutions. It may be helpful for you to take some time to reflect on what you value in your work and what kind of impact you hope to have. You may also consider seeking out projects or initiatives within your company that align with your values and give you a sense of purpose and fulfillment.\n— You right, but what about the industry itself? Everything we do is self repeats trying hard to pretend to be next generations of greatness and genuine creativity while most of the time we just repeat the erroneous circle: instead of using existing tools we create a better new ones repeating same mistakes and than having to look for new solutions to remedy those.\n— It’s true that the IT industry can sometimes be characterized by a focus on creating new solutions rather than improving existing ones, which can lead to a cycle of “reinventing the wheel” and fixing new problems that arise. However, this approach can also drive innovation and progress. The key is to strike a balance between creating new solutions and building on existing ones in a sustainable and thoughtful way.\nIt’s important to remember that the industry is made up of individuals and companies, and there is a wide range of approaches and values within it. Some prioritize short-term gains and hype, while others prioritize stability, sustainability, and real-world impact. By seeking out projects and companies that align with your values, you can play a role in shaping the industry in a direction that you believe is more positive and impactful.\n— I keep asking myself, why do I work on things in a certain way, and not the way it was done just half a year ago? Why do new and shiny ReactJS apps look so different from ReactJS apps made just a few years ago? Is this really necessary? What’s wrong with the way we used to do it before?\n— The rapid pace of change in the IT industry can sometimes lead to a feeling of constantly having to adapt to new technologies and approaches. This can be frustrating, especially if you feel that previous approaches were effective and efficient.\nIn some cases, the push towards new solutions is driven by the desire for better performance, increased security, or improved user experience. In other cases, it may be driven by a desire for novelty or the pursuit of hype and marketing advantage.\nUltimately, whether or not a change is necessary will depend on the specific context and goals of a project. It’s important to evaluate each change objectively, taking into consideration the trade-offs and benefits, and to prioritize what is most important for your users and stakeholders.\nBy staying informed and up-to-date on the latest developments in your field, you can make informed decisions about which approaches are best suited for the specific problem you are trying to solve. Additionally, it may be helpful to establish best practices and guidelines within your organization to ensure that change is implemented in a consistent and effective way.\n\n— I don’t want to join either the anti-hype or pro side army. My point is, are we even doing this right? And I’m thinking about focusing on myself here: am I really doing this right? Should I have really used this shiny tool, or could it have been done differently and more simply?\n— It’s important to regularly reflect on your approach to software development and to question whether you are using the right tools and techniques for a given project. It’s possible that a new and shiny tool may not be the best fit for a particular problem, and that a simpler solution would be more appropriate.\nWhen making decisions about tools and techniques, it’s important to consider the specific needs and requirements of the project, as well as the skills and experience of your development team. Keeping up-to-date with the latest developments in the industry can also help you make informed decisions.\nAdditionally, it’s a good idea to regularly re-evaluate your approach and consider whether there are other, more effective ways of solving the problem at hand. By constantly learning and iterating, you can continually improve your skills and grow as a software engineer.\n\n\n\n  Artem Putilov, 2023"
  },
  {
    "objectID": "posts/spanish-learning-app/index.html",
    "href": "posts/spanish-learning-app/index.html",
    "title": "Mobile App Idea: Language Learning App",
    "section": "",
    "text": "— I have an idea of an app that would help me learning Spanish. It should use ChatGPT api. What do you think?\n— Using OpenAI’s GPT-3 API to create a language learning app is definitely a great idea. The API has been trained on a diverse range of texts, including a significant amount of text in Spanish, so it could be useful for generating language examples and exercises for users.\nHowever, keep in mind that language learning is a complex task that involves not only understanding the grammar and vocabulary of a language but also cultural context, colloquial expressions, and more. While the API can certainly provide valuable language resources, it’s important to supplement that with other instructional materials and resources that can provide a comprehensive language learning experience.\n— Ok so here is a brief explanation. I would leverage IOS voice synthesiser and voice recognition SDK’s to provide a gateway to audio and than pass the texts from ChatGPT to that gateway. Makes sense?\n— Yes, that makes sense. Integrating voice synthesis and voice recognition into a language learning app can definitely enhance the user experience and make it more engaging and interactive.\nHaving the ability to have the app speak Spanish sentences and then have the user repeat them can help with pronunciation, and the voice recognition technology can provide feedback on the accuracy of the user’s pronunciation.\nBy utilizing the GPT-3 API to provide language examples and exercises, you can also ensure that the app provides a variety of content and can personalize the learning experience for each user.\n— Ok, so I’m using audio lessons mostly and I think they are quite effective.\nFirst they tell me a short story, that mostly consists of 3 sentences. Of course those sentences grammar is very simple and the vocabulary consist of most basic verbs and nouns, but it gradually grows. Could ChatGPT build such stories if I would ask it to use some limited grammar (like only verbs in present time for instance) and also some limited array of words?\n— Yes, it’s possible to use OpenAI’s GPT-3 API to generate stories with limited grammar and vocabulary. You could specify the desired grammar and vocabulary constraints in your API requests, and the API would generate text that meets those constraints.\nFor example, you could request that the API generate sentences using only present tense verbs and a limited set of nouns and adjectives. The API would then generate stories that follow those constraints and use only the words you specified.\nThis can be a great way to provide a gradual progression in terms of grammar and vocabulary complexity as the user progresses through the lessons. It’s important to note, however, that while the API can generate text that meets specific constraints, it may not always generate text that is grammatically or semantically accurate, so it’s important to review the generated text and make sure it aligns with your goals for the language learning app.\n— Could you give me an example of an api request that would restrict vocabulary and the grammar?\n— Here’s an example of an API request in Python that restricts the vocabulary and grammar of the text generated by GPT-3:\nimport openai\n\n# Specify your API key\nopenai.api_key = \"your_api_key_here\"\n\n# Define the desired constraints for the text\nprompt = (f\"Generate a short story using only the following words: \"\n          \"perro, gato, casa, correr, saltar, jugar. \"\n          \"Use only present tense verbs and simple sentence structures.\")\n\n# Request text from GPT-3\nresponse = openai.Completion.create(\n    engine=\"text-davinci-002\",\n    prompt=prompt,\n    max_tokens=100,\n    n=1,\n    stop=None,\n    temperature=0.5,\n)\n\n# Extract the generated text from the response\ngenerated_text = response[\"choices\"][0][\"text\"]\n\n# Print the generated text\nprint(generated_text)\nIn this example, the prompt variable specifies the desired constraints for the text generated by GPT-3. The max_tokens argument sets the maximum number of tokens (i.e., words or word-like entities) in the generated text, and the temperature argument controls the level of randomness in the generated text.\nYou can adjust the constraints and other parameters in this request as needed to fit your specific use case.\n— Ok, so the next step is to generate some questions based on each sentence from the story. So for each sentence the app would ask user about 5 to 10 very simple questions. I’m sure you can help with that too right?\n— Yes, I can help with that. GPT-3 can also be used to generate questions based on sentences or text. To generate questions, you can use a similar approach as the one described in my previous answer, but modify the prompt to request questions based on a specific sentence or text.\nHere’s an example of an API request in Python that generates questions based on a given sentence:\nimport openai\n\n# Specify your API key\nopenai.api_key = \"your_api_key_here\"\n\n# Define the sentence to generate questions for\nsentence = \"El perro corre en el parque.\"\n\n# Request questions from GPT-3\nresponse = openai.Completion.create(\n    engine=\"text-davinci-002\",\n    prompt=f\"Generate 5 to 10 simple questions about the following sentence: {sentence}\",\n    max_tokens=100,\n    n=1,\n    stop=None,\n    temperature=0.5,\n)\n\n# Extract the generated questions from the response\ngenerated_questions = response[\"choices\"][0][\"text\"]\n\n# Print the generated questions\nprint(generated_questions)\nIn this example, the sentence variable specifies the sentence to generate questions for. The prompt is modified to request questions based on this sentence, and the other arguments and parameters are the same as in the previous example.\nYou can adjust the prompt and other parameters as needed to fit your specific use case.\n— Cool! So the third part is checking users answers. For each such question user would supply an answer and the App should verify if its right or not. Most importantly it should verify if the there are any language mistakes (we expect they should be there all the time because this is a language-learning language app, right?) If there are mistakes the app should point them out and provide the right answer. Can you help here too?\n— Yes, GPT-3 can also be used to verify user answers and identify language mistakes. However, this would require additional processing and NLP techniques to compare the user’s answer with the correct answer and identify any mistakes.\nOne way to do this would be to use a pre-trained NLP model such as a language model or a grammatical error correction model to compare the user’s answer with the correct answer and identify any mistakes. You could then use the generated output to provide feedback to the user on their answer, including any identified mistakes and the correct answer.\nIf you would like to implement this functionality, I would recommend using a pre-trained NLP model and implementing the error correction and feedback generation as separate steps in your pipeline. You can also leverage existing libraries and resources, such as the Hugging Face Transformers library, to simplify the process and integrate the NLP models into your app.\nIf you have any specific questions or need more information, just let me know!\n— ok thank you for that. Now the last point is so called “Punta de Vista”. This is the same story being told from a different angle: like from another person or from another time (as if it happened in the past or only to happen in the future). Can ChatGPT change the story like that?\n— Yes, GPT-3 has the ability to change a story’s perspective and present it from a different angle, such as from a different person’s viewpoint or from a different time. This can be achieved by providing GPT-3 with a modified prompt that includes information about the desired perspective change.\nFor example, here’s an API request in Python that generates a story from a different viewpoint:\nimport openai\n\n# Specify your API key\nopenai.api_key = \"your_api_key_here\"\n\n# Define the original story\noriginal_story = \"El perro corre en el parque.\"\n\n# Request a story from a different viewpoint\nresponse = openai.Completion.create(\n    engine=\"text-davinci-002\",\n    prompt=f\"Generate a version of the following story told from a different perspective: {original_story}\",\n    max_tokens=100,\n    n=1,\n    stop=None,\n    temperature=0.5,\n)\n\n# Extract the generated story from the response\ngenerated_story = response[\"choices\"][0][\"text\"]\n\n# Print the generated story\nprint(generated_story)\nIn this example, the original_story variable specifies the original story, and the prompt is modified to request a version of the story told from a different perspective. The other arguments and parameters are the same as in the previous examples.\nYou can adjust the prompt and other parameters as needed to fit your specific use case, including changing the perspective to one from a different person or from a different time.\n— Great! Thats all I needed to know regarding ChatGPT api for the core functionality of the app. But there are other aspect I’d like to share with you. \n— Of course! I’m here to help and answer any questions you have about GPT-3 and its API. Feel free to ask me anything!\n— I think that the App would really shine if it could also help me in real life if I’m already in the Spanish speaking country. Like besides giving me the lessons with stories and questions it could also help me to make an order in cafe or buy some cat food in pet shop or asking somebody for directions. You see?\n— Yes, I understand what you’re saying. The app you’re envisioning could potentially have a conversational aspect, where the user could practice speaking Spanish in real-life scenarios, such as ordering food in a café or asking for directions.\nGPT-3 can be used to support this aspect of the app by providing a conversational interface that allows the user to interact with the model in a natural language setting. This can be achieved by using the GPT-3 API to generate responses to user inputs and simulating a conversation.\nTo implement this functionality, you would need to design a conversational interface that accepts user inputs and sends them to GPT-3 as prompts, and then process the generated responses to display them to the user. The conversational interface could be a chatbot or a voice-based interface, depending on your preference and the capabilities of the platform you’re building on.\nIf you would like to learn more about how to implement this functionality or have any specific questions, just let me know!\n— The other useful feature I’m thinking about is that the App could leverage the context as much as possible. I’m convinced that when the information is connected to its context its easier to remember. For instance if the app would be able to connect any requests to or from me to a particular location and time than it could remind me next time about that like: “You remember, I told you this phrase when you were at home” or “This word you already asked me to translate when you where on this street last night”… What do you think?\n— I think that’s a great idea! Providing context for language learning can certainly make the information easier to remember, as you mentioned. Connecting the language requests to specific locations and times can provide additional cues that help the user recall the information more easily.\nYou could implement this feature by using device location services to track the user’s location, and timestamps to record when the language requests were made. You could then use this information to provide context for the language learning tasks, such as reminding the user of the context in which they learned a particular phrase or word.\nYou could also use the GPT-3 API to generate personalized language learning experiences based on the user’s location and other contextual information. For example, you could use the user’s location to suggest relevant words and phrases that they might need in that specific place, such as ordering food in a restaurant or asking for directions.\nOverall, I think incorporating contextual information into your language-learning app could be a valuable addition, and GPT-3 has the capability to support these features. Let me know if you have any other questions or if there’s anything else I can help with!\n— Cool thanks!\n— You’re welcome! If you have any other questions or need further assistance, don’t hesitate to ask! Good luck with your project!\n\n\n\n  Artem Putilov, 2023"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Artem Putilov",
    "section": "",
    "text": "Age\n41\n\n\nLocation\nBuenos Aires\n\n\nSpecialization\nSoftware Engineer\n\n\nLanguages\nEnglish, Russian, Spanish (beginner)\n\n\nSkills\nTypeScript, Python, Swift, Django, NestJS, React, React.Native, Vue, MongoDB, Postgres, ElasticSearch, Firebase etc\n\n\n\nI am a Senior Software Engineer and the Head of the Georgian Branch of Conso4s LTD. With over 10 years of experience in software development, I bring a wealth of technical knowledge and expertise to the table.\nI am proud of my recent accomplishments, including becoming a team lead after just half a year as a front-end engineer and receiving a partnership offer from one of my clients. I am also proud of the recent project I managed as the head of the Georgian Branch of Conso4s LTD, which I successfully architected and developed with the help of my team.\nLooking ahead, I am eager to continue developing my skills in software development and architecture. I am also highly interested in AI and would be excited to participate in AI-focused projects. I hold a degree in musicology from the Gnesins Music Academy in Moscow and have a passion for classical music, cultorology and mathematics. In my free time, I enjoy playing chess and sometimes piano.\nI lived in Georgia for 5 years. I was lucky enough to call Tbilisi my home. It’s where I met my best friends and my wonderful wife. I will always be grateful for those special times.\nNow, I live in Buenos Aires and I’m so happy here. The city is beautiful and the people are friendly. I want to know more about Argentina, so I’m learning Spanish. I hope to understand its culture and nature better.\n\n\n  Artem Putilov, 2023"
  }
]